{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46aeffd",
   "metadata": {},
   "source": [
    "# LunarLander-v2 Agent Training and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3134fbe9",
   "metadata": {},
   "source": [
    "\n",
    "This notebook consolidates all the code needed to implement, train, and visualize a reinforcement learning agent for the `LunarLander-v2` environment. \n",
    "It includes:\n",
    "- Implementation of DQN and Actor-Critic algorithms\n",
    "- Training the agent on the environment\n",
    "- Visualization of training progress\n",
    "- Running and recording agent performance\n",
    "\n",
    "No external `.py` files are needed as all code is integrated into this notebook.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "600c4f8d-7e5b-49cd-a717-8e724613d945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium in c:\\programdata\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install gymnasium numpy torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "109c5315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "import pygame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from torch.distributions import Categorical\n",
    "from gymnasium.wrappers.monitoring import video_recorder\n",
    "\n",
    "import itertools\n",
    "import h5py\n",
    "\n",
    "import os\n",
    "import time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d77d1404-7322-457e-9489-38d55d079adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Box([-90.        -90.         -5.         -5.         -3.1415927  -5.\n",
      "  -0.         -0.       ], [90.        90.         5.         5.         3.1415927  5.\n",
      "  1.         1.       ], (8,), float32)\n",
      "Action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d172bff",
   "metadata": {},
   "source": [
    "## Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ea14913",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") \n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'mps'\n",
    "import warnings\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class memory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class neural_network(nn.Module):\n",
    "    '''\n",
    "    Feedforward neural network with variable number\n",
    "    of hidden layers and ReLU nonlinearites\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                layers=[8,64,32,4],# layers[i] = # of neurons at i-th layer\n",
    "                # layers[0] = input layer\n",
    "                # layers[-1] = output layer\n",
    "                dropout=False,\n",
    "                p_dropout=0.5,\n",
    "                ):\n",
    "        super(neural_network,self).__init__()\n",
    "\n",
    "        self.network_layers = []\n",
    "        n_layers = len(layers)\n",
    "        for i,neurons_in_current_layer in enumerate(layers[:-1]):\n",
    "            #\n",
    "            self.network_layers.append(nn.Linear(neurons_in_current_layer, \n",
    "                                                layers[i+1]) )\n",
    "            #\n",
    "            if dropout:\n",
    "                self.network_layers.append( nn.Dropout(p=p_dropout) )\n",
    "            #\n",
    "            if i < n_layers - 2:\n",
    "                self.network_layers.append( nn.ReLU() )\n",
    "        #\n",
    "        self.network_layers = nn.Sequential(*self.network_layers)\n",
    "        #\n",
    "\n",
    "    def forward(self,x):\n",
    "        for layer in self.network_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class agent_base():\n",
    "\n",
    "    def __init__(self,parameters):\n",
    "        \"\"\"\n",
    "        Initializes the agent class\n",
    "\n",
    "        Keyword arguments:\n",
    "        parameters -- dictionary with parameters for the agent\n",
    "\n",
    "        There are two mandatory keys for the dictionary:\n",
    "        - N_state (int): dimensionality of the (continuous) state space\n",
    "        - N_actions (int): number of actions available to the agent\n",
    "\n",
    "        All other arguments are optional, for a list see the class methods \n",
    "        get_default_parameters(self,parameters)\n",
    "        set_parameters(self,parameters)\n",
    "\n",
    "        \"\"\"\n",
    "        #\n",
    "        parameters = self.make_dictionary_keys_lowercase(parameters)\n",
    "        #\n",
    "        # set parameters that are mandatory and can only be set at \n",
    "        # initializaton of a class instance\n",
    "        self.set_initialization_parameters(parameters=parameters)\n",
    "        #\n",
    "        # get dictionary with default parameters\n",
    "        default_parameters = self.get_default_parameters()\n",
    "        # for all parameters not set by the input dictionary, add the \n",
    "        # respective default parameter\n",
    "        parameters = self.merge_dictionaries(dict1=parameters,\n",
    "                                             dict2=default_parameters)\n",
    "        # set all parameters (except for those already set above in \n",
    "        # self.set_initialization_parameters())\n",
    "        self.set_parameters(parameters=parameters)\n",
    "        #\n",
    "        # for future reference, each instance of a class carries a copy of \n",
    "        # the parameters as internal variable\n",
    "        self.parameters = copy.deepcopy(parameters)\n",
    "        #\n",
    "        # intialize neural networks \n",
    "        self.initialize_neural_networks(neural_networks=\\\n",
    "                                            parameters['neural_networks'])\n",
    "        # initialize the optimizer and loss function used for training\n",
    "        self.initialize_optimizers(optimizers=parameters['optimizers'])\n",
    "        self.initialize_losses(losses=parameters['losses'])\n",
    "        #\n",
    "        self.in_training = False\n",
    "\n",
    "    def make_dictionary_keys_lowercase(self,dictionary):\n",
    "        output_dictionary = {}\n",
    "        for key, value in dictionary.items():\n",
    "            output_dictionary[key.lower()] = value\n",
    "        return output_dictionary\n",
    "\n",
    "    def merge_dictionaries(self,dict1,dict2):\n",
    "        '''\n",
    "        Merge two dictionaries and return the merged dictionary\n",
    "\n",
    "        If a key \"key\" exists in both dict1 and dict2, then the value from\n",
    "        dict1 is used for the returned dictionary\n",
    "        '''\n",
    "        #\n",
    "        return_dict = copy.deepcopy(dict1)\n",
    "        #\n",
    "        dict1_keys = return_dict.keys()\n",
    "        for key, value in dict2.items():\n",
    "            # we just add those entries from dict2 to dict1\n",
    "            # that do not already exist in dict1\n",
    "            if key not in dict1_keys:\n",
    "                return_dict[key] = value\n",
    "        #\n",
    "        return return_dict\n",
    "\n",
    "    def get_default_parameters(self):\n",
    "        '''\n",
    "        Create and return dictionary with the default parameters of the class\n",
    "        '''\n",
    "        #\n",
    "        parameters = {\n",
    "            'neural_networks':\n",
    "                {\n",
    "                'policy_net':{\n",
    "                    'layers':[self.n_state,128,32,self.n_actions],\n",
    "                            }\n",
    "                },\n",
    "            'optimizers':\n",
    "                {\n",
    "                'policy_net':{\n",
    "                    'optimizer':'RMSprop',\n",
    "                     'optimizer_args':{'lr':1e-3}, # learning rate\n",
    "                            }\n",
    "                },\n",
    "            'losses':\n",
    "                {\n",
    "                'policy_net':{            \n",
    "                    'loss':'MSELoss',\n",
    "                }\n",
    "                },\n",
    "            #\n",
    "            'n_memory':20000,\n",
    "            'training_stride':5,\n",
    "            'batch_size':32,\n",
    "            'saving_stride':100,\n",
    "            #\n",
    "            'n_episodes_max':10000,\n",
    "            'n_solving_episodes':20,\n",
    "            'solving_threshold_min':200,\n",
    "            'solving_threshold_mean':230,\n",
    "            #\n",
    "            'discount_factor':0.99,\n",
    "            }\n",
    "        #\n",
    "        # in case at some point the above dictionary is edited and an upper\n",
    "        # case key is added:\n",
    "        parameters = self.make_dictionary_keys_lowercase(parameters)\n",
    "        #\n",
    "        return parameters\n",
    "\n",
    "\n",
    "    def set_initialization_parameters(self,parameters):\n",
    "        '''Set those class parameters that are required at initialization'''\n",
    "        #\n",
    "        try: # set mandatory parameter N_state\n",
    "            self.n_state = parameters['n_state']\n",
    "        except KeyError:\n",
    "            raise RuntimeError(\"Parameter N_state (= # of input\"\\\n",
    "                         +\" nodes for neural network) needs to be supplied.\")\n",
    "        #\n",
    "        try: # set mandatory parameter N_actions\n",
    "            self.n_actions = parameters['n_actions']\n",
    "        except KeyError:\n",
    "            raise RuntimeError(\"Parameter N_actions (= # of output\"\\\n",
    "                         +\" nodes for neural network) needs to be supplied.\")\n",
    "\n",
    "    def set_parameters(self,parameters):\n",
    "        \"\"\"Set training parameters\"\"\"\n",
    "        #\n",
    "        parameters = self.make_dictionary_keys_lowercase(parameters)\n",
    "        #\n",
    "        ########################################\n",
    "        # Discount factor for Bellman equation #\n",
    "        ########################################\n",
    "        try: # \n",
    "            self.discount_factor = parameters['discount_factor']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        #\n",
    "        #################################\n",
    "        # Experience replay memory size #\n",
    "        #################################\n",
    "        try: # \n",
    "            self.n_memory = int(parameters['n_memory'])\n",
    "            self.memory = memory(self.n_memory)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        #\n",
    "        ###############################\n",
    "        # Parameters for optimization #\n",
    "        ###############################\n",
    "        try: # number of simulation timesteps between optimization steps\n",
    "            self.training_stride = parameters['training_stride']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        #\n",
    "        try: # size of mini-batch for each optimization step\n",
    "            self.batch_size = int(parameters['batch_size'])\n",
    "        except KeyError:\n",
    "            pass\n",
    "        #\n",
    "        try: # IO during training: every saving_stride episodes, the \n",
    "            # current status of the training is saved to disk\n",
    "            self.saving_stride = parameters['saving_stride']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        #\n",
    "        ##############################################\n",
    "        # Parameters for training stopping criterion #\n",
    "        ##############################################\n",
    "        try: # maximal number of episodes until the training is stopped \n",
    "            # (if stopping criterion is not met before)\n",
    "            self.n_episodes_max = parameters['n_episodes_max']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        #\n",
    "        try: # # of the last N_solving episodes that need to fulfill the\n",
    "            # stopping criterion for minimal and mean episode return\n",
    "            self.n_solving_episodes = parameters['n_solving_episodes']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        #\n",
    "        try: # minimal return over last N_solving_episodes\n",
    "            self.solving_threshold_min = parameters['solving_threshold_min']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        #\n",
    "        try: # mean return over last N_solving_episodes\n",
    "            self.solving_threshold_mean = parameters['solving_threshold_mean']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        #\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"Return dictionary with parameters of the current agent instance\"\"\"\n",
    "\n",
    "        return self.parameters\n",
    "\n",
    "    def initialize_neural_networks(self,neural_networks):\n",
    "        \"\"\"Initialize all neural networks\"\"\"\n",
    "\n",
    "        self.neural_networks = {}\n",
    "        for key, value in neural_networks.items():\n",
    "            self.neural_networks[key] = neural_network(value['layers']).to(device)\n",
    "        \n",
    "    def initialize_optimizers(self,optimizers):\n",
    "        \"\"\"Initialize optimizers\"\"\"\n",
    "\n",
    "        self.optimizers = {}\n",
    "        for key, value in optimizers.items():\n",
    "            self.optimizers[key] = torch.optim.RMSprop(\n",
    "                        self.neural_networks[key].parameters(),\n",
    "                            **value['optimizer_args'])\n",
    "    \n",
    "    def initialize_losses(self,losses):\n",
    "        \"\"\"Instantiate loss functions\"\"\"\n",
    "\n",
    "        self.losses = {}\n",
    "        for key, value in losses.items():\n",
    "            self.losses[key] = nn.MSELoss()\n",
    "\n",
    "    def get_number_of_model_parameters(self,name='policy_net'): \n",
    "        \"\"\"Return the number of trainable neural network parameters\"\"\"\n",
    "        # from https://stackoverflow.com/a/49201237\n",
    "        return sum(p.numel() for p in self.neural_networks[name].parameters() \\\n",
    "                                    if p.requires_grad)\n",
    "\n",
    "\n",
    "    def get_state(self):\n",
    "        '''Return dictionary with current state of neural net and optimizer'''\n",
    "        #\n",
    "        state = {'parameters':self.get_parameters()}\n",
    "        #\n",
    "        for name,neural_network in self.neural_networks.items():\n",
    "            state[name] = copy.deepcopy(neural_network.state_dict())\n",
    "        #\n",
    "        for name,optimizer in (self.optimizers).items():\n",
    "            #\n",
    "            state[name+'_optimizer'] = copy.deepcopy(optimizer.state_dict())\n",
    "        #\n",
    "        return state\n",
    "    \n",
    "\n",
    "    def load_state(self,state):\n",
    "        '''\n",
    "        Load given states for neural networks and optimizer\n",
    "\n",
    "        The argument \"state\" has to be a dictionary with the following \n",
    "        (key, value) pairs:\n",
    "\n",
    "        1. state['parameters'] = dictionary with the agents parameters\n",
    "        2. For every neural network, there should be a state dictionary:\n",
    "            state['$name'] = state dictionary of neural_network['$name']\n",
    "        3. For every optimizer, there should be a state dictionary:\n",
    "            state['$name_optimizer'] = state dictionary of optimizers['$name']\n",
    "        '''\n",
    "        #\n",
    "        parameters=state['parameters']\n",
    "        #\n",
    "        self.check_parameter_dictionary_compatibility(parameters=parameters)\n",
    "        #\n",
    "        self.__init__(parameters=parameters)\n",
    "        #\n",
    "        #\n",
    "        for name,state_dict in (state).items():\n",
    "            if name == 'parameters':\n",
    "                continue\n",
    "            elif 'optimizer' in name:\n",
    "                name = name.replace('_optimizer','')\n",
    "                self.optimizers[name].load_state_dict(state_dict)\n",
    "            else:\n",
    "                self.neural_networks[name].load_state_dict(state_dict)\n",
    "        #\n",
    "\n",
    "\n",
    "    def check_parameter_dictionary_compatibility(self,parameters):\n",
    "        \"\"\"Check compatibility of provided parameter dictionary with class\"\"\"\n",
    "\n",
    "        error_string = (\"Error loading state. Provided parameter {0} = {1} \",\n",
    "                    \"is inconsistent with agent class parameter {0} = {2}. \",\n",
    "                    \"Please instantiate a new agent class with parameters\",\n",
    "                    \" matching those of the model you would like to load.\")\n",
    "        try: \n",
    "            n_state =  parameters['n_state']\n",
    "            if n_state != self.n_state:\n",
    "                raise RuntimeError(error_string.format('n_state',n_state,\n",
    "                                                self.n_state))\n",
    "        except KeyError:\n",
    "            pass\n",
    "        #\n",
    "        try: \n",
    "            n_actions =  parameters['n_actions']\n",
    "            if n_actions != self.n_actions:\n",
    "                raise RuntimeError(error_string.format('n_actions',n_actions,\n",
    "                                                self.n_actions))\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def evaluate_stopping_criterion(self,list_of_returns):\n",
    "        \"\"\" Evaluate stopping criterion \"\"\"\n",
    "        # if we have run at least self.N_solving_episodes, check\n",
    "        # whether the stopping criterion is met\n",
    "        if len(list_of_returns) < self.n_solving_episodes:\n",
    "            return False, 0., 0.\n",
    "        #\n",
    "        # get numpy array with recent returns\n",
    "        recent_returns = np.array(list_of_returns)\n",
    "        recent_returns = recent_returns[-self.n_solving_episodes:]\n",
    "        #\n",
    "        # calculate minimal and mean return over the last\n",
    "        # self.n_solving_episodes epsiodes \n",
    "        minimal_return = np.min(recent_returns)\n",
    "        mean_return = np.mean(recent_returns)\n",
    "        #\n",
    "        # check whether stopping criterion is met\n",
    "        if minimal_return > self.solving_threshold_min:\n",
    "            if mean_return > self.solving_threshold_mean:\n",
    "                return True, minimal_return, mean_return\n",
    "        # if stopping crtierion is not met:\n",
    "        return False, minimal_return, mean_return\n",
    "\n",
    "\n",
    "    def act(self,state):\n",
    "        \"\"\"\n",
    "        Select an action for the current state\n",
    "        \"\"\"\n",
    "        #\n",
    "        # This typically uses the policy net. See the child classes below\n",
    "        # for examples:\n",
    "        # - dqn: makes decisions using an epsilon-greedy algorithm\n",
    "        # - actor_critic: draws a stochastic decision with probabilities given\n",
    "        #                 by the current stochastic policy\n",
    "        #\n",
    "        # As an example, we here draw a fully random action:\n",
    "        return np.random.randint(self.n_actions) \n",
    "\n",
    "\n",
    "    def add_memory(self,memory):\n",
    "        \"\"\"Add current experience tuple to the memory\"\"\"\n",
    "        self.memory.push(*memory)\n",
    "\n",
    "    def get_samples_from_memory(self):\n",
    "        '''\n",
    "        Get a tuple (states, actions, next_states, rewards, episode_end? ) \n",
    "        from the memory, as appopriate for experience replay\n",
    "        '''\n",
    "        #\n",
    "        # get random sample of transitions from memory\n",
    "        current_transitions = self.memory.sample(batch_size=self.batch_size)\n",
    "        #\n",
    "        # convert list of Transition elements to Transition element with lists\n",
    "        # (see https://stackoverflow.com/a/19343/3343043)\n",
    "        batch = Transition(*zip(*current_transitions))\n",
    "        #\n",
    "        # convert lists of current transitions to torch tensors\n",
    "        state_batch = torch.cat( [s.unsqueeze(0) for s in batch.state],\n",
    "                                        dim=0)#.to(device)\n",
    "        # state_batch.shape = [batch_size, N_states]\n",
    "        next_state_batch = torch.cat(\n",
    "                         [s.unsqueeze(0) for s in batch.next_state],dim=0)\n",
    "        action_batch = torch.cat(batch.action)#.to(device)\n",
    "        # action_batch.shape = [batch_size]\n",
    "        reward_batch = torch.cat(batch.reward)#.to(device)\n",
    "        done_batch = torch.tensor(batch.done).float()#.to(device)\n",
    "        #\n",
    "        return state_batch, action_batch, next_state_batch, \\\n",
    "                        reward_batch, done_batch\n",
    "\n",
    "\n",
    "    def run_optimization_step(self, epoch):\n",
    "        \"\"\"Run one optimization step\n",
    "        \n",
    "        Keyword argument:\n",
    "        epoch (int) -- number of current training epoch\n",
    "        \"\"\"\n",
    "        #\n",
    "        # Here is where the actual optimization happens.\n",
    "        # \n",
    "        # This method MUST be implemented in any child class, and might look\n",
    "        # very different depending on the learning algorithm. \n",
    "        # Note that any implementation must contain the argument \"epoch\", as \n",
    "        # this method is called as run_optimization_step(epoch=epoch) in the\n",
    "        # method self.train() below.\n",
    "        # \n",
    "        # For examples see the child classes \"dqn\" and \"actor_critic\" below\n",
    "        #\n",
    "        \n",
    "    \n",
    "\n",
    "    def train(self,environment,\n",
    "                    verbose=True,\n",
    "                    model_filename=None,\n",
    "                    training_filename=None,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Train the agent on a provided environment\n",
    "\n",
    "        Keyword arguments:\n",
    "        environment -- environment used by the agent to train. This should be\n",
    "                       an instance of a class with methods \"reset\" and \"step\".\n",
    "                       - environment.reset() should reset the environment to\n",
    "                         an initial state and return a tuple,\n",
    "                            current_state, info = environment.reset(),\n",
    "                         such current_state is an initial state of the with\n",
    "                         np.shape(current_state) = (self.N_state,)\n",
    "                       - environment.set(action) should take an integer in \n",
    "                         {0, ..., self.N_action-1} and return a tuple, \n",
    "                            s, r, te, tr, info = environment.step(action),\n",
    "                         where s is the next state with shape (self.N_state,),\n",
    "                         r is the current reward (a float), and where te and\n",
    "                         tr are two Booleans that tell us whether the episode\n",
    "                         has terminated (te == True) or has been truncated \n",
    "                         (tr == True)\n",
    "        verbose (Bool) -- Print progress of training to terminal. Defaults to\n",
    "                          True\n",
    "        model_filename (string) -- Output filename for final trained model and\n",
    "                                   periodic snapshots of the model during \n",
    "                                   training. Defaults to None, in which case\n",
    "                                   nothing is not written to disk\n",
    "        training_filename (string) -- Output filename for training data, \n",
    "                                      namely lists of episode durations, \n",
    "                                      episode returns, number of training \n",
    "                                      epochs, and total number of steps \n",
    "                                      simulated. Defaults to None, in which \n",
    "                                      case no training data is written to disk\n",
    "        \"\"\"\n",
    "        self.in_training = True\n",
    "        #\n",
    "        training_complete = False\n",
    "        step_counter = 0 # total number of simulated environment steps\n",
    "        epoch_counter = 0 # number of training epochs \n",
    "        #\n",
    "        # lists for documenting the training\n",
    "        episode_durations = [] # duration of each training episodes\n",
    "        episode_returns = [] # return of each training episode\n",
    "        steps_simulated = [] # total number of steps simulated at the end of\n",
    "                             # each training episode\n",
    "        training_epochs = [] # total number of training epochs at the end of \n",
    "                             # each training episode\n",
    "        #\n",
    "        output_state_dicts = {} # dictionary in which we will save the status\n",
    "                                # of the neural networks and optimizer\n",
    "                                # every self.saving_stride steps epochs during\n",
    "                                # training. \n",
    "                                # We also store the final neural network\n",
    "                                # resulting from our training in this \n",
    "                                # dictionary\n",
    "        #\n",
    "        if verbose:\n",
    "            training_progress_header = (\n",
    "                \"| episode | return          | minimal return    \"\n",
    "                    \"  | mean return        |\\n\"\n",
    "                \"|         | (this episode)  | (last {0} episodes)  \"\n",
    "                    \"| (last {0} episodes) |\\n\"\n",
    "                \"|---------------------------------------------------\"\n",
    "                    \"--------------------\")\n",
    "            print(training_progress_header.format(self.n_solving_episodes))\n",
    "            #\n",
    "            status_progress_string = ( # for outputting status during training\n",
    "                        \"| {0: 7d} |   {1: 10.3f}    |     \"\n",
    "                        \"{2: 10.3f}      |    {3: 10.3f}      |\")\n",
    "        #\n",
    "        for n_episode in range(self.n_episodes_max):\n",
    "            #\n",
    "            # reset environment and reward of current episode\n",
    "            state, info = environment.reset()\n",
    "            current_total_reward = 0.\n",
    "            #\n",
    "            for i in itertools.count(): # timesteps of environment\n",
    "                #\n",
    "                # select action using policy net\n",
    "                action = self.act(state=state)\n",
    "                #\n",
    "                # perform action\n",
    "                next_state, reward, terminated, truncated, info = \\\n",
    "                                        environment.step(action)\n",
    "                #\n",
    "                step_counter += 1 # increase total steps simulated\n",
    "                done = terminated or truncated # did the episode end?\n",
    "                current_total_reward += reward # add current reward to total\n",
    "                #\n",
    "                # store the transition in memory\n",
    "                reward = torch.tensor([np.float32(reward)], device=device)\n",
    "                action = torch.tensor([action], device=device)\n",
    "                self.add_memory([torch.tensor(state),\n",
    "                            action,\n",
    "                            torch.tensor(next_state),\n",
    "                            reward,\n",
    "                            done])\n",
    "                #\n",
    "                state = next_state\n",
    "                #\n",
    "                if step_counter % self.training_stride == 0:\n",
    "                    # train model\n",
    "                    self.run_optimization_step(epoch=epoch_counter) # optimize\n",
    "                    epoch_counter += 1 # increase count of optimization steps\n",
    "                #\n",
    "                if done: # if current episode ended\n",
    "                    #\n",
    "                    # update training statistics\n",
    "                    episode_durations.append(i + 1)\n",
    "                    episode_returns.append(current_total_reward)\n",
    "                    steps_simulated.append(step_counter)\n",
    "                    training_epochs.append(epoch_counter)\n",
    "                    #\n",
    "                    # check whether the stopping criterion is met\n",
    "                    training_complete, min_ret, mean_ret = \\\n",
    "                            self.evaluate_stopping_criterion(\\\n",
    "                                list_of_returns=episode_returns)\n",
    "                    if verbose:\n",
    "                            # print training stats\n",
    "                            if n_episode % 100 == 0 and n_episode > 0:\n",
    "                                end='\\n'\n",
    "                            else:\n",
    "                                end='\\r'\n",
    "                            if min_ret > self.solving_threshold_min:\n",
    "                                if mean_ret > self.solving_threshold_mean:\n",
    "                                    end='\\n'\n",
    "                            #\n",
    "                            print(status_progress_string.format(n_episode,\n",
    "                                    current_total_reward,\n",
    "                                   min_ret,mean_ret),\n",
    "                                        end=end)\n",
    "                    break\n",
    "            #\n",
    "            # Save model and training stats to disk\n",
    "            if (n_episode % self.saving_stride == 0) \\\n",
    "                    or training_complete \\\n",
    "                    or n_episode == self.n_episodes_max-1:\n",
    "                #\n",
    "                if model_filename != None:\n",
    "                    output_state_dicts[n_episode] = self.get_state()\n",
    "                    torch.save(output_state_dicts, model_filename)\n",
    "                #\n",
    "                training_results = {'episode_durations':episode_durations,\n",
    "                            'epsiode_returns':episode_returns,\n",
    "                            'n_training_epochs':training_epochs,\n",
    "                            'n_steps_simulated':steps_simulated,\n",
    "                            'training_completed':False,\n",
    "                            }\n",
    "                if training_filename != None:\n",
    "                    self.save_dictionary(dictionary=training_results,\n",
    "                                        filename=training_filename)\n",
    "            #\n",
    "            if training_complete:\n",
    "                # we stop if the stopping criterion was met at the end of\n",
    "                # the current episode\n",
    "                training_results['training_completed'] = True\n",
    "                break\n",
    "        #\n",
    "        if not training_complete:\n",
    "            # if we stopped the training because the maximal number of\n",
    "            # episodes was reached, we throw a warning\n",
    "            warning_string = (\"Warning: Training was stopped because the \"\n",
    "            \"maximum number of episodes, {0}, was reached. But the stopping \"\n",
    "            \"criterion has not been met.\")\n",
    "            warnings.warn(warning_string.format(self.n_episodes_max))\n",
    "        #\n",
    "        self.in_training = False\n",
    "        #\n",
    "        return training_results\n",
    "\n",
    "    def save_dictionary(self,dictionary,filename):\n",
    "        \"\"\"Save a dictionary in hdf5 format\"\"\"\n",
    "\n",
    "        with h5py.File(filename, 'w') as hf:\n",
    "            self.save_dictionary_recursively(h5file=hf,\n",
    "                                            path='/',\n",
    "                                            dictionary=dictionary)\n",
    "                \n",
    "    def save_dictionary_recursively(self,h5file,path,dictionary):\n",
    "        #\n",
    "        \"\"\"\n",
    "        slightly adapted from https://codereview.stackexchange.com/a/121308\n",
    "        \"\"\"\n",
    "        for key, value in dictionary.items():\n",
    "            if isinstance(value, dict):\n",
    "                self.save_dictionary_recursively(h5file, \n",
    "                                                path + str(key) + '/',\n",
    "                                                value)\n",
    "            else:\n",
    "                h5file[path + str(key)] = value\n",
    "\n",
    "    def load_dictionary(self,filename):\n",
    "        with h5py.File(filename, 'r') as hf:\n",
    "            return self.load_dictionary_recursively(h5file=hf,\n",
    "                                                    path='/')\n",
    "\n",
    "    def load_dictionary_recursively(self,h5file, path):\n",
    "        \"\"\"\n",
    "        From https://codereview.stackexchange.com/a/121308\n",
    "        \"\"\"\n",
    "        return_dict = {}\n",
    "        for key, value in h5file[path].items():\n",
    "            if isinstance(value, h5py._hl.dataset.Dataset):\n",
    "                return_dict[key] = value.value\n",
    "            elif isinstance(value, h5py._hl.group.Group):\n",
    "                return_dict[key] = self.load_dictionary_recursively(\\\n",
    "                                            h5file=h5file, \n",
    "                                            path=path + key + '/')\n",
    "        return return_dict\n",
    "\n",
    "\n",
    "\n",
    "class dqn(agent_base):\n",
    "\n",
    "    def __init__(self,parameters):\n",
    "        super().__init__(parameters=parameters)\n",
    "        self.in_training = False\n",
    "\n",
    "    def get_default_parameters(self):\n",
    "        '''\n",
    "        Create and return dictionary with the default parameters of the dqn\n",
    "        algorithm\n",
    "        '''\n",
    "        #\n",
    "        default_parameters = super().get_default_parameters()\n",
    "        #\n",
    "        # add default parameters specific to the dqn algorithm\n",
    "        default_parameters['neural_networks']['target_net'] = {}\n",
    "        default_parameters['neural_networks']['target_net']['layers'] = \\\n",
    "        copy.deepcopy(\\\n",
    "                default_parameters['neural_networks']['policy_net']['layers'])\n",
    "        #\n",
    "        #\n",
    "        # soft update stride for target net:\n",
    "        default_parameters['target_net_update_stride'] = 1 \n",
    "        # soft update parameter for target net:\n",
    "        default_parameters['target_net_update_tau'] = 1e-2 \n",
    "        #\n",
    "        # Parameters for epsilon-greedy policy with epoch-dependent epsilon\n",
    "        default_parameters['epsilon'] = 1.0 # initial value for epsilon\n",
    "        default_parameters['epsilon_1'] = 0.1 # final value for epsilon\n",
    "        default_parameters['d_epsilon'] = 0.00005 # decrease of epsilon\n",
    "            # after each training epoch\n",
    "        #\n",
    "        default_parameters['doubledqn'] = False\n",
    "        #\n",
    "        return default_parameters\n",
    "\n",
    "\n",
    "    def set_parameters(self,parameters):\n",
    "        #\n",
    "        super().set_parameters(parameters=parameters)\n",
    "        #\n",
    "        ##################################################\n",
    "        # Use deep Q-learning or double deep Q-learning? #\n",
    "        ##################################################\n",
    "        try: # False -> use DQN; True -> use double DQN\n",
    "            self.doubleDQN = parameters['doubledqn']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        #\n",
    "        ##########################################\n",
    "        # Parameters for updating the target net #\n",
    "        ##########################################\n",
    "        try: # after how many training epochs do we update the target net?\n",
    "            self.target_net_update_stride = \\\n",
    "                                    parameters['target_net_update_stride']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        #\n",
    "        try: # tau for soft update of target net (value 1 means hard update)\n",
    "            self.target_net_update_tau = parameters['target_net_update_tau']\n",
    "            # check if provided parameter is within bounds\n",
    "            error_msg = (\"Parameter 'target_net_update_tau' has to be \"\n",
    "                    \"between 0 and 1, but value {0} has been passed.\")\n",
    "            error_msg = error_msg.format(self.target_net_update_tau)\n",
    "            if self.target_net_update_tau < 0:\n",
    "                raise RuntimeError(error_msg)\n",
    "            elif self.target_net_update_tau > 1:\n",
    "                raise RuntimeError(error_msg)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        #\n",
    "        #\n",
    "        ########################################\n",
    "        # Parameters for epsilon-greedy policy #\n",
    "        ########################################\n",
    "        try: # probability for random action for epsilon-greedy policy\n",
    "            self.epsilon = \\\n",
    "                    parameters['epsilon']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        #\n",
    "        try: # final probability for random action during training \n",
    "            #  for epsilon-greedy policy\n",
    "            self.epsilon_1 = \\\n",
    "                    parameters['epsilon_1']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        # \n",
    "        try: # amount by which epsilon decreases during each training epoch\n",
    "            #  until the final value self.epsilon_1 is reached\n",
    "            self.d_epsilon = \\\n",
    "                    parameters['d_epsilon']\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    def act(self,state,epsilon=0.):\n",
    "        \"\"\"\n",
    "        Use policy net to select an action for the current state\n",
    "        \n",
    "        We use an epsilon-greedy algorithm: \n",
    "        - With probability epsilon we take a random action (uniformly drawn\n",
    "          from the finite number of available actions)\n",
    "        - With probability 1-epsilon we take the optimal action (as predicted\n",
    "          by the policy net)\n",
    "\n",
    "        By default epsilon = 0, which means that we actually use the greedy \n",
    "        algorithm for action selection\n",
    "        \"\"\"\n",
    "        #\n",
    "        if self.in_training:\n",
    "            epsilon = self.epsilon\n",
    "\n",
    "        if torch.rand(1).item() > epsilon:\n",
    "            # \n",
    "            policy_net = self.neural_networks['policy_net']\n",
    "            #\n",
    "            with torch.no_grad():\n",
    "                policy_net.eval()\n",
    "                action = policy_net(torch.tensor(state)).argmax(0).item()\n",
    "                policy_net.train()\n",
    "                return action\n",
    "        else:\n",
    "            # perform random action\n",
    "            return torch.randint(low=0,high=self.n_actions,size=(1,)).item()\n",
    "        \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"\n",
    "        Update epsilon for epsilon-greedy algorithm\n",
    "        \n",
    "        For training we assume that \n",
    "        epsilon(n) = max{ epsilon_0 - d_epsilon * n ,  epsilon_1 },\n",
    "        where n is the number of training epochs.\n",
    "\n",
    "        For epsilon_0 > epsilon_1 the function epsilon(n) is piecewise linear.\n",
    "        It first decreases from epsilon_0 to epsilon_1 with a slope d_epsilon,\n",
    "        and then becomes constant at the value epsilon_1.\n",
    "        \n",
    "        This ensures that during the initial phase of training the neural \n",
    "        network explores more randomly, and in later stages of the training\n",
    "        follows more the policy learned by the neural net.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon - self.d_epsilon, self.epsilon_1)\n",
    "\n",
    "    def run_optimization_step(self,epoch):\n",
    "        \"\"\"Run one optimization step for the policy net\"\"\"\n",
    "        #\n",
    "        # if we have less sample transitions than we would draw in an \n",
    "        # optimization step, we do nothing\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        #\n",
    "        state_batch, action_batch, next_state_batch, \\\n",
    "                        reward_batch, done_batch = self.get_samples_from_memory()\n",
    "        #\n",
    "        policy_net = self.neural_networks['policy_net']\n",
    "        target_net = self.neural_networks['target_net']\n",
    "        #\n",
    "        optimizer = self.optimizers['policy_net']\n",
    "        loss = self.losses['policy_net']\n",
    "        #\n",
    "        policy_net.train() # turn on training mode\n",
    "        #\n",
    "        # Evaluate left-hand side of the Bellman equation using policy net\n",
    "        LHS = policy_net(state_batch.to(device)).gather(dim=1,\n",
    "                                 index=action_batch.unsqueeze(1))\n",
    "        # LHS.shape = [batch_size, 1]\n",
    "        #\n",
    "        # Evaluate right-hand side of Bellman equation\n",
    "        if self.doubleDQN:\n",
    "            # double deep-Q learning paper: https://arxiv.org/abs/1509.06461\n",
    "            #\n",
    "            # in double deep Q-learning, we use the policy net for choosing\n",
    "            # the action on the right-hand side of the Bellman equation. We \n",
    "            # then use the target net to evaluate the Q-function on the \n",
    "            # chosen action\n",
    "            argmax_next_state = policy_net(next_state_batch).argmax(\n",
    "                                                                    dim=1)\n",
    "            # argmax_next_state.shape = [batch_size]\n",
    "            #\n",
    "            Q_next_state = target_net(next_state_batch).gather(\n",
    "                dim=1,index=argmax_next_state.unsqueeze(1)).squeeze(1)\n",
    "            # shapes of the various tensor appearing in the previous line:\n",
    "            # self.target_net(next_state_batch).shape = [batch_size,N_actions]\n",
    "            # self.target_net(next_state_batch).gather(dim=1,\n",
    "            #   index=argmax_next_state.unsqueeze(1)).shape = [batch_size, 1]\n",
    "            # Q_next_state.shape = [batch_size]\n",
    "        else:\n",
    "            # in deep Q-learning, we use the target net both for choosing\n",
    "            # the action on the right-hand side of the Bellman equation, and \n",
    "            # for evaluating the Q-function on that action\n",
    "            Q_next_state = target_net(next_state_batch\\\n",
    "                                                ).max(1)[0].detach()\n",
    "            # Q_next_state.shape = [batch_size]\n",
    "        RHS = Q_next_state * self.discount_factor * (1.-done_batch) \\\n",
    "                            + reward_batch\n",
    "        RHS = RHS.unsqueeze(1) # RHS.shape = [batch_size, 1]\n",
    "        #\n",
    "        # optimize the model\n",
    "        loss_ = loss(LHS, RHS)\n",
    "        optimizer.zero_grad()\n",
    "        loss_.backward()\n",
    "        optimizer.step()\n",
    "        #\n",
    "        policy_net.eval() # turn off training mode\n",
    "        #\n",
    "        self.update_epsilon() # for epsilon-greedy algorithm\n",
    "        #\n",
    "        if epoch % self.target_net_update_stride == 0:\n",
    "            self.soft_update_target_net() # soft update target net\n",
    "        #\n",
    "        \n",
    "    def soft_update_target_net(self):\n",
    "        \"\"\"Soft update parameters of target net\"\"\"\n",
    "        #\n",
    "        # the following code is from https://stackoverflow.com/q/48560227\n",
    "        params1 = self.neural_networks['policy_net'].named_parameters()\n",
    "        params2 = self.neural_networks['target_net'].named_parameters()\n",
    "\n",
    "        dict_params2 = dict(params2)\n",
    "\n",
    "        for name1, param1 in params1:\n",
    "            if name1 in dict_params2:\n",
    "                dict_params2[name1].data.copy_(\\\n",
    "                    self.target_net_update_tau*param1.data\\\n",
    "                + (1-self.target_net_update_tau)*dict_params2[name1].data)\n",
    "        self.neural_networks['target_net'].load_state_dict(dict_params2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class actor_critic(agent_base):\n",
    "    #\n",
    "\n",
    "    def __init__(self,parameters):\n",
    "\n",
    "        super().__init__(parameters=parameters)\n",
    "\n",
    "        #\n",
    "        self.Softmax = nn.Softmax(dim=0)\n",
    "        self.LogSoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def get_default_parameters(self):\n",
    "        #\n",
    "        default_parameters = super().get_default_parameters()\n",
    "        #\n",
    "        # add default parameters specific to the dqn algorithm\n",
    "        default_parameters['neural_networks']['critic_net'] = {}\n",
    "        default_parameters['neural_networks']['critic_net']['layers'] = \\\n",
    "                    [self.n_state,64,32,1] # needs to have scalar output\n",
    "        #\n",
    "        default_parameters['optimizers']['critic_net'] = {\n",
    "                    'optimizer':'RMSprop',\n",
    "                     'optimizer_args':{'lr':1e-3}, # learning rate\n",
    "                            }\n",
    "        #\n",
    "        default_parameters['affinities_regularization'] = 0.01\n",
    "        #\n",
    "        return default_parameters\n",
    "    \n",
    "    def set_parameters(self,parameters):\n",
    "        #\n",
    "        super().set_parameters(parameters=parameters)\n",
    "        #\n",
    "        try: \n",
    "            self.affinities_regularization = \\\n",
    "                            parameters['affinities_regularization']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        #\n",
    "\n",
    "    def initialize_losses(self,losses):\n",
    "        \"\"\"Instantiate loss class\n",
    "        \n",
    "        Note that the argument \"losses\" is mandatory, even though it is not\n",
    "        used for the particular class\n",
    "        \"\"\"\n",
    "\n",
    "        # for the actor we need a custom loss function\n",
    "        def loss_actor(state_batch,action_batch,advantage_batch):\n",
    "            affinities = self.neural_networks['policy_net'](state_batch)\n",
    "            #\n",
    "            log_pi_a = self.LogSoftmax(affinities).gather(dim=1,\n",
    "                                    index=action_batch.unsqueeze(1))\n",
    "            loss_actor = -log_pi_a * advantage_batch \\\n",
    "                            + self.affinities_regularization \\\n",
    "                                *torch.sum(affinities**2)/self.batch_size\n",
    "            loss_actor = loss_actor.sum()\n",
    "            return loss_actor\n",
    "\n",
    "        self.losses = {}\n",
    "        self.losses['policy_net'] = loss_actor\n",
    "        self.losses['critic_net'] = nn.MSELoss()\n",
    "\n",
    "    def act(self,state):\n",
    "        \"\"\"\n",
    "        Use policy net to select an action for the current state\n",
    "\n",
    "        For the actor-critic algorithm, the actor chooses an action\n",
    "        from the available actions\n",
    "            {1, .., n_action}\n",
    "        according to their (stochastic) policy.\n",
    "        More explicitly, for each state s the policy yields a vector of \n",
    "        probabilities \n",
    "            pi(s) = (pi_1, ..., pi_{n_action})\n",
    "        for the n_action actions. The actor draws an action according to these\n",
    "        probabilities pi(s).\n",
    "        \"\"\"\n",
    "        actor_net = self.neural_networks['policy_net']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            actor_net.eval()\n",
    "            # see\n",
    "            #https://pytorch.org/docs/stable/distributions.html#score-function\n",
    "            probs = self.Softmax(actor_net(torch.tensor(state)))\n",
    "            m = Categorical(probs)\n",
    "            action = m.sample()\n",
    "            actor_net.train()\n",
    "            return action.item()\n",
    "        \n",
    "    def run_optimization_step(self,epoch):\n",
    "        \"\"\"Run one optimization step for the policy net\"\"\"\n",
    "        #\n",
    "        # Note that the parameter \"epoch\" is not actually used here, but it is\n",
    "        # a mandatory parameter because the the method train() in the base \n",
    "        # class calls run_optimization_step(epoch=epoch).\n",
    "        #\n",
    "        ################################\n",
    "        # Draw experiences from memory #\n",
    "        ################################\n",
    "        # If we have less sample transitions than we would draw in an \n",
    "        # optimization step, we do nothing\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        #\n",
    "        state_batch, action_batch, next_state_batch, \\\n",
    "                    reward_batch, done_batch = self.get_samples_from_memory()\n",
    "        #\n",
    "        ###################################################################\n",
    "        # Define local names for neural networks, optimizers, and losses  #\n",
    "        ###################################################################\n",
    "        actor_net = self.neural_networks['policy_net']\n",
    "        critic_net = self.neural_networks['critic_net']\n",
    "        #\n",
    "        optimizer_actor = self.optimizers['policy_net']\n",
    "        optimizer_critic = self.optimizers['critic_net']\n",
    "        #\n",
    "        loss_actor = self.losses['policy_net']\n",
    "        loss_critic = self.losses['critic_net']\n",
    "        #\n",
    "        ################\n",
    "        # train critic #\n",
    "        ################\n",
    "        critic_net.train() # turn on training mode\n",
    "        #\n",
    "        # Evaluate left-hand side of the Bellman equation using policy net\n",
    "        LHS = critic_net(state_batch.to(device))\n",
    "        # LHS.shape = [batch_size, 1]\n",
    "        Q_next_state = critic_net(next_state_batch).detach().squeeze(1)\n",
    "        RHS = Q_next_state * self.discount_factor * (1.-done_batch) \\\n",
    "                            + reward_batch\n",
    "        RHS = RHS.unsqueeze(1) # RHS.shape = [batch_size, 1]\n",
    "        #\n",
    "        # optimize the model\n",
    "        loss = loss_critic(LHS, RHS)\n",
    "        optimizer_critic.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_critic.step()\n",
    "        #\n",
    "        critic_net.eval() # turn off training mode\n",
    "        #\n",
    "        ###############\n",
    "        # train actor #\n",
    "        ###############\n",
    "        actor_net.train()\n",
    "        advantage_batch = (RHS - LHS).detach()\n",
    "        loss = loss_actor(state_batch=state_batch,\n",
    "                          action_batch=action_batch,\n",
    "                          advantage_batch=advantage_batch)\n",
    "        optimizer_actor.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_actor.step()\n",
    "        #\n",
    "        actor_net.eval()\n",
    "        #\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5dcfc2",
   "metadata": {},
   "source": [
    "## Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70cabcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of state space = 8\n",
      "number of actions = 4\n",
      "| episode | return          | minimal return      | mean return        |\n",
      "|         | (this episode)  | (last 20 episodes)  | (last 20 episodes) |\n",
      "|-----------------------------------------------------------------------\n",
      "|      63 |     -133.353    |       -518.440      |      -171.636      |\r"
     ]
    }
   ],
   "source": [
    "# Define settings directly for Jupyter or script-based use\n",
    "output_filename = 'my_agent.tar'\n",
    "output_filename_training_data = 'my_agent_training_data.h5'\n",
    "output_filename_time = 'my_agent_execution_time.txt'\n",
    "verbose = True\n",
    "overwrite = True\n",
    "use_dqn = True  # Set to True for DQN, False for Actor-Critic\n",
    "use_ddqn = False  # Set to True for Double DQN (overrides DQN)\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Obtain dimensions of action and observation space\n",
    "N_actions = env.action_space.n\n",
    "observation, info = env.reset()\n",
    "N_state = len(observation)\n",
    "if verbose:\n",
    "    print('dimension of state space =', N_state)\n",
    "    print('number of actions =', N_actions)\n",
    "\n",
    "# Check if files already exist (if overwrite is False)\n",
    "if not overwrite:\n",
    "    error_msg = (\"File {0} already exists. If you want to overwrite\"\n",
    "                 \" that file, please set overwrite = True.\")\n",
    "    if os.path.exists(output_filename):\n",
    "        raise RuntimeError(error_msg.format(output_filename))\n",
    "    if os.path.exists(output_filename_training_data):\n",
    "        raise RuntimeError(error_msg.format(output_filename_training_data))\n",
    "\n",
    "# Set parameters\n",
    "parameters = {\n",
    "    'N_state': N_state,\n",
    "    'N_actions': N_actions,\n",
    "    'discount_factor': 0.99,  # discount factor for Bellman equation\n",
    "    'N_memory': 20000,  # number of past transitions stored in memory\n",
    "    'training_stride': 5,  # number of simulation timesteps between optimizations\n",
    "    'batch_size': 32,  # mini-batch size for optimizer\n",
    "    'saving_stride': 100,  # save the model every this many episodes\n",
    "    'n_episodes_max': 10000,  # maximum episodes for training\n",
    "    'n_solving_episodes': 20,  # number of episodes for stopping criteria\n",
    "    'solving_threshold_min': 200.,  # minimal return threshold\n",
    "    'solving_threshold_mean': 230.,  # mean return threshold\n",
    "}\n",
    "\n",
    "# Instantiate agent class\n",
    "if use_dqn or use_ddqn:\n",
    "    if use_ddqn:\n",
    "        parameters['doubledqn'] = True\n",
    "    try:\n",
    "        # Make sure dqn class is defined or imported\n",
    "        my_agent = dqn(parameters=parameters)  # dqn class should be defined in the script\n",
    "    except NameError:\n",
    "        print(\"Error: 'dqn' is not defined. Ensure the class is correctly included or imported.\")\n",
    "else:\n",
    "    try:\n",
    "        # Make sure actor_critic class is defined or imported\n",
    "        my_agent = actor_critic(parameters=parameters)  # actor_critic class should be defined in the script\n",
    "    except NameError:\n",
    "        print(\"Error: 'actor_critic' is not defined. Ensure the class is correctly included or imported.\")\n",
    "\n",
    "# Train agent on environment\n",
    "start_time = time.time()\n",
    "training_results = my_agent.train(\n",
    "    environment=env,\n",
    "    verbose=verbose,\n",
    "    model_filename=output_filename,\n",
    "    training_filename=output_filename_training_data,\n",
    ")\n",
    "execution_time = time.time() - start_time\n",
    "with open(output_filename_time, 'w') as f:\n",
    "    f.write(str(execution_time))\n",
    "\n",
    "if verbose:\n",
    "    print('Execution time in seconds: ' + str(execution_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368a03ec",
   "metadata": {},
   "source": [
    "## Running Simulations and Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8d56088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanke\\AppData\\Local\\Temp\\ipykernel_29120\\4034107332.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  input_dictionary = torch.load(open(input_filename, 'rb'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 of 1000 completed with return 243.9. Mean return over all episodes so far = 243.9 \n",
      "Run 2 of 1000 completed with return 267.7. Mean return over all episodes so far = 255.8 \n",
      "Run 3 of 1000 completed with return 273.6. Mean return over all episodes so far = 261.7 \n",
      "Run 4 of 1000 completed with return 247.3. Mean return over all episodes so far = 258.1 \n",
      "Run 5 of 1000 completed with return 258.0. Mean return over all episodes so far = 258.1 \n",
      "Run 6 of 1000 completed with return 262.2. Mean return over all episodes so far = 258.8 \n",
      "Run 7 of 1000 completed with return 245.2. Mean return over all episodes so far = 256.9 \n",
      "Run 8 of 1000 completed with return 254.7. Mean return over all episodes so far = 256.6 \n",
      "Run 9 of 1000 completed with return 272.1. Mean return over all episodes so far = 258.3 \n",
      "Run 10 of 1000 completed with return 249.2. Mean return over all episodes so far = 257.4 \n",
      "Run 11 of 1000 completed with return 238.1. Mean return over all episodes so far = 255.6 \n",
      "Run 12 of 1000 completed with return 251.6. Mean return over all episodes so far = 255.3 \n",
      "Run 13 of 1000 completed with return 258.6. Mean return over all episodes so far = 255.6 \n",
      "Run 14 of 1000 completed with return 242.4. Mean return over all episodes so far = 254.6 \n",
      "Run 15 of 1000 completed with return 244.5. Mean return over all episodes so far = 253.9 \n",
      "Run 16 of 1000 completed with return 262.5. Mean return over all episodes so far = 254.5 \n",
      "Run 17 of 1000 completed with return 253.8. Mean return over all episodes so far = 254.4 \n",
      "Run 18 of 1000 completed with return 238.3. Mean return over all episodes so far = 253.5 \n",
      "Run 19 of 1000 completed with return 265.8. Mean return over all episodes so far = 254.2 \n",
      "Run 20 of 1000 completed with return 267.0. Mean return over all episodes so far = 254.8 \n",
      "Run 21 of 1000 completed with return 263.4. Mean return over all episodes so far = 255.2 \n",
      "Run 22 of 1000 completed with return 270.9. Mean return over all episodes so far = 256.0 \n",
      "Run 23 of 1000 completed with return 228.0. Mean return over all episodes so far = 254.7 \n",
      "Run 24 of 1000 completed with return 292.4. Mean return over all episodes so far = 256.3 \n",
      "Run 25 of 1000 completed with return 250.9. Mean return over all episodes so far = 256.1 \n",
      "Run 26 of 1000 completed with return 253.1. Mean return over all episodes so far = 256.0 \n",
      "Run 27 of 1000 completed with return 265.5. Mean return over all episodes so far = 256.3 \n",
      "Run 28 of 1000 completed with return 236.1. Mean return over all episodes so far = 255.6 \n",
      "Run 29 of 1000 completed with return 235.6. Mean return over all episodes so far = 254.9 \n",
      "Run 30 of 1000 completed with return 259.9. Mean return over all episodes so far = 255.1 \n",
      "Run 31 of 1000 completed with return -67.0. Mean return over all episodes so far = 244.7 \n",
      "Run 32 of 1000 completed with return 233.4. Mean return over all episodes so far = 244.3 \n",
      "Run 33 of 1000 completed with return 256.7. Mean return over all episodes so far = 244.7 \n",
      "Run 34 of 1000 completed with return 146.0. Mean return over all episodes so far = 241.8 \n",
      "Run 35 of 1000 completed with return 251.2. Mean return over all episodes so far = 242.1 \n",
      "Run 36 of 1000 completed with return 231.0. Mean return over all episodes so far = 241.8 \n",
      "Run 37 of 1000 completed with return 286.4. Mean return over all episodes so far = 243.0 \n",
      "Run 38 of 1000 completed with return 272.1. Mean return over all episodes so far = 243.8 \n",
      "Run 39 of 1000 completed with return 96.5 . Mean return over all episodes so far = 240.0 \n",
      "Run 40 of 1000 completed with return 278.9. Mean return over all episodes so far = 241.0 \n",
      "Run 41 of 1000 completed with return 284.1. Mean return over all episodes so far = 242.0 \n",
      "Run 42 of 1000 completed with return 256.5. Mean return over all episodes so far = 242.3 \n",
      "Run 43 of 1000 completed with return 249.6. Mean return over all episodes so far = 242.5 \n",
      "Run 44 of 1000 completed with return 238.0. Mean return over all episodes so far = 242.4 \n",
      "Run 45 of 1000 completed with return 264.9. Mean return over all episodes so far = 242.9 \n",
      "Run 46 of 1000 completed with return 255.6. Mean return over all episodes so far = 243.2 \n",
      "Run 47 of 1000 completed with return 285.2. Mean return over all episodes so far = 244.1 \n",
      "Run 48 of 1000 completed with return 253.5. Mean return over all episodes so far = 244.3 \n",
      "Run 49 of 1000 completed with return 255.9. Mean return over all episodes so far = 244.5 \n",
      "Run 50 of 1000 completed with return 269.2. Mean return over all episodes so far = 245.0 \n",
      "Run 51 of 1000 completed with return 245.0. Mean return over all episodes so far = 245.0 \n",
      "Run 52 of 1000 completed with return 259.5. Mean return over all episodes so far = 245.3 \n",
      "Run 53 of 1000 completed with return 236.8. Mean return over all episodes so far = 245.1 \n",
      "Run 54 of 1000 completed with return 258.1. Mean return over all episodes so far = 245.4 \n",
      "Run 55 of 1000 completed with return 230.2. Mean return over all episodes so far = 245.1 \n",
      "Run 56 of 1000 completed with return 257.8. Mean return over all episodes so far = 245.3 \n",
      "Run 57 of 1000 completed with return 281.2. Mean return over all episodes so far = 245.9 \n",
      "Run 58 of 1000 completed with return 233.6. Mean return over all episodes so far = 245.7 \n",
      "Run 59 of 1000 completed with return 244.5. Mean return over all episodes so far = 245.7 \n",
      "Run 60 of 1000 completed with return 258.5. Mean return over all episodes so far = 245.9 \n",
      "Run 61 of 1000 completed with return 234.5. Mean return over all episodes so far = 245.7 \n",
      "Run 62 of 1000 completed with return 262.5. Mean return over all episodes so far = 246.0 \n",
      "Run 63 of 1000 completed with return 271.1. Mean return over all episodes so far = 246.4 \n",
      "Run 64 of 1000 completed with return 262.0. Mean return over all episodes so far = 246.7 \n",
      "Run 65 of 1000 completed with return 255.7. Mean return over all episodes so far = 246.8 \n",
      "Run 66 of 1000 completed with return 255.7. Mean return over all episodes so far = 246.9 \n",
      "Run 67 of 1000 completed with return 245.7. Mean return over all episodes so far = 246.9 \n",
      "Run 68 of 1000 completed with return 270.7. Mean return over all episodes so far = 247.3 \n",
      "Run 69 of 1000 completed with return 248.2. Mean return over all episodes so far = 247.3 \n",
      "Run 70 of 1000 completed with return 285.5. Mean return over all episodes so far = 247.8 \n",
      "Run 71 of 1000 completed with return 140.0. Mean return over all episodes so far = 246.3 \n",
      "Run 72 of 1000 completed with return 283.4. Mean return over all episodes so far = 246.8 \n",
      "Run 73 of 1000 completed with return 262.5. Mean return over all episodes so far = 247.0 \n",
      "Run 74 of 1000 completed with return 237.8. Mean return over all episodes so far = 246.9 \n",
      "Run 75 of 1000 completed with return 267.7. Mean return over all episodes so far = 247.2 \n",
      "Run 76 of 1000 completed with return 249.9. Mean return over all episodes so far = 247.2 \n",
      "Run 77 of 1000 completed with return 240.3. Mean return over all episodes so far = 247.1 \n",
      "Run 78 of 1000 completed with return 270.9. Mean return over all episodes so far = 247.4 \n",
      "Run 79 of 1000 completed with return 269.4. Mean return over all episodes so far = 247.7 \n",
      "Run 80 of 1000 completed with return 263.1. Mean return over all episodes so far = 247.9 \n",
      "Run 81 of 1000 completed with return 276.3. Mean return over all episodes so far = 248.3 \n",
      "Run 82 of 1000 completed with return 266.0. Mean return over all episodes so far = 248.5 \n",
      "Run 83 of 1000 completed with return 286.3. Mean return over all episodes so far = 248.9 \n",
      "Run 84 of 1000 completed with return 252.6. Mean return over all episodes so far = 249.0 \n",
      "Run 85 of 1000 completed with return 248.7. Mean return over all episodes so far = 249.0 \n",
      "Run 86 of 1000 completed with return 122.0. Mean return over all episodes so far = 247.5 \n",
      "Run 87 of 1000 completed with return 230.7. Mean return over all episodes so far = 247.3 \n",
      "Run 88 of 1000 completed with return 242.4. Mean return over all episodes so far = 247.2 \n",
      "Run 89 of 1000 completed with return 277.3. Mean return over all episodes so far = 247.6 \n",
      "Run 90 of 1000 completed with return 269.8. Mean return over all episodes so far = 247.8 \n",
      "Run 91 of 1000 completed with return 270.0. Mean return over all episodes so far = 248.1 \n",
      "Run 92 of 1000 completed with return 248.5. Mean return over all episodes so far = 248.1 \n",
      "Run 93 of 1000 completed with return 230.8. Mean return over all episodes so far = 247.9 \n",
      "Run 94 of 1000 completed with return 234.9. Mean return over all episodes so far = 247.8 \n",
      "Run 95 of 1000 completed with return 252.5. Mean return over all episodes so far = 247.8 \n",
      "Run 96 of 1000 completed with return 272.3. Mean return over all episodes so far = 248.1 \n",
      "Run 97 of 1000 completed with return 135.4. Mean return over all episodes so far = 246.9 \n",
      "Run 98 of 1000 completed with return 232.1. Mean return over all episodes so far = 246.7 \n",
      "Run 99 of 1000 completed with return 262.4. Mean return over all episodes so far = 246.9 \n",
      "Run 100 of 1000 completed with return 261.7. Mean return over all episodes so far = 247.1 \n",
      "Run 101 of 1000 completed with return 287.2. Mean return over all episodes so far = 247.4 \n",
      "Run 102 of 1000 completed with return 264.2. Mean return over all episodes so far = 247.6 \n",
      "Run 103 of 1000 completed with return 266.4. Mean return over all episodes so far = 247.8 \n",
      "Run 104 of 1000 completed with return 272.8. Mean return over all episodes so far = 248.0 \n",
      "Run 105 of 1000 completed with return 242.1. Mean return over all episodes so far = 248.0 \n",
      "Run 106 of 1000 completed with return 280.3. Mean return over all episodes so far = 248.3 \n",
      "Run 107 of 1000 completed with return 249.0. Mean return over all episodes so far = 248.3 \n",
      "Run 108 of 1000 completed with return 229.3. Mean return over all episodes so far = 248.1 \n",
      "Run 109 of 1000 completed with return 262.9. Mean return over all episodes so far = 248.3 \n",
      "Run 110 of 1000 completed with return 264.3. Mean return over all episodes so far = 248.4 \n",
      "Run 111 of 1000 completed with return 292.4. Mean return over all episodes so far = 248.8 \n",
      "Run 112 of 1000 completed with return 270.1. Mean return over all episodes so far = 249.0 \n",
      "Run 113 of 1000 completed with return 130.0. Mean return over all episodes so far = 247.9 \n",
      "Run 114 of 1000 completed with return 248.7. Mean return over all episodes so far = 247.9 \n",
      "Run 115 of 1000 completed with return 267.7. Mean return over all episodes so far = 248.1 \n",
      "Run 116 of 1000 completed with return 265.6. Mean return over all episodes so far = 248.3 \n",
      "Run 117 of 1000 completed with return 260.7. Mean return over all episodes so far = 248.4 \n",
      "Run 118 of 1000 completed with return 245.8. Mean return over all episodes so far = 248.3 \n",
      "Run 119 of 1000 completed with return 250.2. Mean return over all episodes so far = 248.4 \n",
      "Run 120 of 1000 completed with return 119.0. Mean return over all episodes so far = 247.3 \n",
      "Run 121 of 1000 completed with return -10.2. Mean return over all episodes so far = 245.2 \n",
      "Run 122 of 1000 completed with return 272.9. Mean return over all episodes so far = 245.4 \n",
      "Run 123 of 1000 completed with return 267.0. Mean return over all episodes so far = 245.6 \n",
      "Run 124 of 1000 completed with return 122.3. Mean return over all episodes so far = 244.6 \n",
      "Run 125 of 1000 completed with return 247.2. Mean return over all episodes so far = 244.6 \n",
      "Run 126 of 1000 completed with return 250.9. Mean return over all episodes so far = 244.6 \n",
      "Run 127 of 1000 completed with return 233.8. Mean return over all episodes so far = 244.5 \n",
      "Run 128 of 1000 completed with return 267.0. Mean return over all episodes so far = 244.7 \n",
      "Run 129 of 1000 completed with return 250.3. Mean return over all episodes so far = 244.8 \n",
      "Run 130 of 1000 completed with return 266.8. Mean return over all episodes so far = 244.9 \n",
      "Run 131 of 1000 completed with return 201.5. Mean return over all episodes so far = 244.6 \n",
      "Run 132 of 1000 completed with return 245.5. Mean return over all episodes so far = 244.6 \n",
      "Run 133 of 1000 completed with return 232.8. Mean return over all episodes so far = 244.5 \n",
      "Run 134 of 1000 completed with return 261.0. Mean return over all episodes so far = 244.6 \n",
      "Run 135 of 1000 completed with return 257.9. Mean return over all episodes so far = 244.7 \n",
      "Run 136 of 1000 completed with return 228.7. Mean return over all episodes so far = 244.6 \n",
      "Run 137 of 1000 completed with return 124.9. Mean return over all episodes so far = 243.8 \n",
      "Run 138 of 1000 completed with return 262.1. Mean return over all episodes so far = 243.9 \n",
      "Run 139 of 1000 completed with return 225.4. Mean return over all episodes so far = 243.8 \n",
      "Run 140 of 1000 completed with return 98.5 . Mean return over all episodes so far = 242.7 \n",
      "Run 141 of 1000 completed with return 265.0. Mean return over all episodes so far = 242.9 \n",
      "Run 142 of 1000 completed with return 241.0. Mean return over all episodes so far = 242.9 \n",
      "Run 143 of 1000 completed with return 155.4. Mean return over all episodes so far = 242.2 \n",
      "Run 144 of 1000 completed with return 262.0. Mean return over all episodes so far = 242.4 \n",
      "Run 145 of 1000 completed with return 286.5. Mean return over all episodes so far = 242.7 \n",
      "Run 146 of 1000 completed with return 280.2. Mean return over all episodes so far = 242.9 \n",
      "Run 147 of 1000 completed with return 238.5. Mean return over all episodes so far = 242.9 \n",
      "Run 148 of 1000 completed with return 264.2. Mean return over all episodes so far = 243.1 \n",
      "Run 149 of 1000 completed with return 286.2. Mean return over all episodes so far = 243.4 \n",
      "Run 150 of 1000 completed with return 255.5. Mean return over all episodes so far = 243.4 \n",
      "Run 151 of 1000 completed with return 273.7. Mean return over all episodes so far = 243.6 \n",
      "Run 152 of 1000 completed with return 277.2. Mean return over all episodes so far = 243.9 \n",
      "Run 153 of 1000 completed with return 241.6. Mean return over all episodes so far = 243.8 \n",
      "Run 154 of 1000 completed with return 243.3. Mean return over all episodes so far = 243.8 \n",
      "Run 155 of 1000 completed with return 256.5. Mean return over all episodes so far = 243.9 \n",
      "Run 156 of 1000 completed with return 235.9. Mean return over all episodes so far = 243.9 \n",
      "Run 157 of 1000 completed with return 227.7. Mean return over all episodes so far = 243.8 \n",
      "Run 158 of 1000 completed with return 274.3. Mean return over all episodes so far = 244.0 \n",
      "Run 159 of 1000 completed with return 240.8. Mean return over all episodes so far = 243.9 \n",
      "Run 160 of 1000 completed with return 261.3. Mean return over all episodes so far = 244.0 \n",
      "Run 161 of 1000 completed with return 260.2. Mean return over all episodes so far = 244.1 \n",
      "Run 162 of 1000 completed with return 243.8. Mean return over all episodes so far = 244.1 \n",
      "Run 163 of 1000 completed with return 272.1. Mean return over all episodes so far = 244.3 \n",
      "Run 164 of 1000 completed with return 289.0. Mean return over all episodes so far = 244.6 \n",
      "Run 165 of 1000 completed with return 263.1. Mean return over all episodes so far = 244.7 \n",
      "Run 166 of 1000 completed with return 250.8. Mean return over all episodes so far = 244.7 \n",
      "Run 167 of 1000 completed with return 160.3. Mean return over all episodes so far = 244.2 \n",
      "Run 168 of 1000 completed with return 238.7. Mean return over all episodes so far = 244.2 \n",
      "Run 169 of 1000 completed with return 240.8. Mean return over all episodes so far = 244.2 \n",
      "Run 170 of 1000 completed with return 293.1. Mean return over all episodes so far = 244.5 \n",
      "Run 171 of 1000 completed with return 253.7. Mean return over all episodes so far = 244.5 \n",
      "Run 172 of 1000 completed with return 243.1. Mean return over all episodes so far = 244.5 \n",
      "Run 173 of 1000 completed with return 247.3. Mean return over all episodes so far = 244.5 \n",
      "Run 174 of 1000 completed with return 280.6. Mean return over all episodes so far = 244.7 \n",
      "Run 175 of 1000 completed with return 223.9. Mean return over all episodes so far = 244.6 \n",
      "Run 176 of 1000 completed with return 250.3. Mean return over all episodes so far = 244.6 \n",
      "Run 177 of 1000 completed with return 238.7. Mean return over all episodes so far = 244.6 \n",
      "Run 178 of 1000 completed with return 238.8. Mean return over all episodes so far = 244.6 \n",
      "Run 179 of 1000 completed with return 242.6. Mean return over all episodes so far = 244.6 \n",
      "Run 180 of 1000 completed with return 281.5. Mean return over all episodes so far = 244.8 \n",
      "Run 181 of 1000 completed with return 248.7. Mean return over all episodes so far = 244.8 \n",
      "Run 182 of 1000 completed with return 258.0. Mean return over all episodes so far = 244.9 \n",
      "Run 183 of 1000 completed with return 272.8. Mean return over all episodes so far = 245.0 \n",
      "Run 184 of 1000 completed with return 221.6. Mean return over all episodes so far = 244.9 \n",
      "Run 185 of 1000 completed with return 259.4. Mean return over all episodes so far = 245.0 \n",
      "Run 186 of 1000 completed with return 245.8. Mean return over all episodes so far = 245.0 \n",
      "Run 187 of 1000 completed with return 252.5. Mean return over all episodes so far = 245.0 \n",
      "Run 188 of 1000 completed with return 258.2. Mean return over all episodes so far = 245.1 \n",
      "Run 189 of 1000 completed with return 244.5. Mean return over all episodes so far = 245.1 \n",
      "Run 190 of 1000 completed with return 244.1. Mean return over all episodes so far = 245.1 \n",
      "Run 191 of 1000 completed with return 292.4. Mean return over all episodes so far = 245.3 \n",
      "Run 192 of 1000 completed with return 275.0. Mean return over all episodes so far = 245.5 \n",
      "Run 193 of 1000 completed with return 216.7. Mean return over all episodes so far = 245.3 \n",
      "Run 194 of 1000 completed with return 222.9. Mean return over all episodes so far = 245.2 \n",
      "Run 195 of 1000 completed with return 281.2. Mean return over all episodes so far = 245.4 \n",
      "Run 196 of 1000 completed with return 282.6. Mean return over all episodes so far = 245.6 \n",
      "Run 197 of 1000 completed with return 248.1. Mean return over all episodes so far = 245.6 \n",
      "Run 198 of 1000 completed with return 265.6. Mean return over all episodes so far = 245.7 \n",
      "Run 199 of 1000 completed with return 257.1. Mean return over all episodes so far = 245.8 \n",
      "Run 200 of 1000 completed with return 262.3. Mean return over all episodes so far = 245.8 \n",
      "Run 201 of 1000 completed with return 244.4. Mean return over all episodes so far = 245.8 \n",
      "Run 202 of 1000 completed with return 257.4. Mean return over all episodes so far = 245.9 \n",
      "Run 203 of 1000 completed with return 271.5. Mean return over all episodes so far = 246.0 \n",
      "Run 204 of 1000 completed with return 256.5. Mean return over all episodes so far = 246.1 \n",
      "Run 205 of 1000 completed with return 261.4. Mean return over all episodes so far = 246.1 \n",
      "Run 206 of 1000 completed with return 290.7. Mean return over all episodes so far = 246.4 \n",
      "Run 207 of 1000 completed with return 259.4. Mean return over all episodes so far = 246.4 \n",
      "Run 208 of 1000 completed with return 273.2. Mean return over all episodes so far = 246.6 \n",
      "Run 209 of 1000 completed with return 237.6. Mean return over all episodes so far = 246.5 \n",
      "Run 210 of 1000 completed with return 259.4. Mean return over all episodes so far = 246.6 \n",
      "Run 211 of 1000 completed with return 270.9. Mean return over all episodes so far = 246.7 \n",
      "Run 212 of 1000 completed with return 261.8. Mean return over all episodes so far = 246.8 \n",
      "Run 213 of 1000 completed with return 245.7. Mean return over all episodes so far = 246.8 \n",
      "Run 214 of 1000 completed with return 272.9. Mean return over all episodes so far = 246.9 \n",
      "Run 215 of 1000 completed with return 272.0. Mean return over all episodes so far = 247.0 \n",
      "Run 216 of 1000 completed with return 245.3. Mean return over all episodes so far = 247.0 \n",
      "Run 217 of 1000 completed with return 290.9. Mean return over all episodes so far = 247.2 \n",
      "Run 218 of 1000 completed with return 252.2. Mean return over all episodes so far = 247.2 \n",
      "Run 219 of 1000 completed with return 255.8. Mean return over all episodes so far = 247.3 \n",
      "Run 220 of 1000 completed with return 37.3 . Mean return over all episodes so far = 246.3 \n",
      "Run 221 of 1000 completed with return 285.8. Mean return over all episodes so far = 246.5 \n",
      "Run 222 of 1000 completed with return 207.0. Mean return over all episodes so far = 246.3 \n",
      "Run 223 of 1000 completed with return 269.2. Mean return over all episodes so far = 246.4 \n",
      "Run 224 of 1000 completed with return 254.4. Mean return over all episodes so far = 246.4 \n",
      "Run 225 of 1000 completed with return 267.7. Mean return over all episodes so far = 246.5 \n",
      "Run 226 of 1000 completed with return 239.8. Mean return over all episodes so far = 246.5 \n",
      "Run 227 of 1000 completed with return 267.8. Mean return over all episodes so far = 246.6 \n",
      "Run 228 of 1000 completed with return 242.2. Mean return over all episodes so far = 246.6 \n",
      "Run 229 of 1000 completed with return 287.5. Mean return over all episodes so far = 246.8 \n",
      "Run 230 of 1000 completed with return 33.6 . Mean return over all episodes so far = 245.8 \n",
      "Run 231 of 1000 completed with return 259.7. Mean return over all episodes so far = 245.9 \n",
      "Run 232 of 1000 completed with return 234.2. Mean return over all episodes so far = 245.8 \n",
      "Run 233 of 1000 completed with return 252.6. Mean return over all episodes so far = 245.9 \n",
      "Run 234 of 1000 completed with return 223.2. Mean return over all episodes so far = 245.8 \n",
      "Run 235 of 1000 completed with return 242.4. Mean return over all episodes so far = 245.8 \n",
      "Run 236 of 1000 completed with return 289.6. Mean return over all episodes so far = 245.9 \n",
      "Run 237 of 1000 completed with return 251.2. Mean return over all episodes so far = 246.0 \n",
      "Run 238 of 1000 completed with return 267.6. Mean return over all episodes so far = 246.1 \n",
      "Run 239 of 1000 completed with return 247.3. Mean return over all episodes so far = 246.1 \n",
      "Run 240 of 1000 completed with return 260.2. Mean return over all episodes so far = 246.1 \n",
      "Run 241 of 1000 completed with return 276.9. Mean return over all episodes so far = 246.2 \n",
      "Run 242 of 1000 completed with return 248.9. Mean return over all episodes so far = 246.3 \n",
      "Run 243 of 1000 completed with return 258.7. Mean return over all episodes so far = 246.3 \n",
      "Run 244 of 1000 completed with return 152.5. Mean return over all episodes so far = 245.9 \n",
      "Run 245 of 1000 completed with return 264.8. Mean return over all episodes so far = 246.0 \n",
      "Run 246 of 1000 completed with return 239.3. Mean return over all episodes so far = 246.0 \n",
      "Run 247 of 1000 completed with return 265.5. Mean return over all episodes so far = 246.1 \n",
      "Run 248 of 1000 completed with return 270.6. Mean return over all episodes so far = 246.2 \n",
      "Run 249 of 1000 completed with return 245.7. Mean return over all episodes so far = 246.1 \n",
      "Run 250 of 1000 completed with return 231.8. Mean return over all episodes so far = 246.1 \n",
      "Run 251 of 1000 completed with return 275.7. Mean return over all episodes so far = 246.2 \n",
      "Run 252 of 1000 completed with return 246.0. Mean return over all episodes so far = 246.2 \n",
      "Run 253 of 1000 completed with return 258.9. Mean return over all episodes so far = 246.3 \n",
      "Run 254 of 1000 completed with return 270.7. Mean return over all episodes so far = 246.4 \n",
      "Run 255 of 1000 completed with return 258.8. Mean return over all episodes so far = 246.4 \n",
      "Run 256 of 1000 completed with return 243.5. Mean return over all episodes so far = 246.4 \n",
      "Run 257 of 1000 completed with return 243.6. Mean return over all episodes so far = 246.4 \n",
      "Run 258 of 1000 completed with return 126.9. Mean return over all episodes so far = 245.9 \n",
      "Run 259 of 1000 completed with return 256.5. Mean return over all episodes so far = 246.0 \n",
      "Run 260 of 1000 completed with return 250.3. Mean return over all episodes so far = 246.0 \n",
      "Run 261 of 1000 completed with return 239.5. Mean return over all episodes so far = 246.0 \n",
      "Run 262 of 1000 completed with return 294.7. Mean return over all episodes so far = 246.1 \n",
      "Run 263 of 1000 completed with return 293.4. Mean return over all episodes so far = 246.3 \n",
      "Run 264 of 1000 completed with return 257.3. Mean return over all episodes so far = 246.4 \n",
      "Run 265 of 1000 completed with return 287.3. Mean return over all episodes so far = 246.5 \n",
      "Run 266 of 1000 completed with return 254.3. Mean return over all episodes so far = 246.5 \n",
      "Run 267 of 1000 completed with return 253.3. Mean return over all episodes so far = 246.6 \n",
      "Run 268 of 1000 completed with return 127.5. Mean return over all episodes so far = 246.1 \n",
      "Run 269 of 1000 completed with return 287.6. Mean return over all episodes so far = 246.3 \n",
      "Run 270 of 1000 completed with return 255.9. Mean return over all episodes so far = 246.3 \n",
      "Run 271 of 1000 completed with return 242.8. Mean return over all episodes so far = 246.3 \n",
      "Run 272 of 1000 completed with return 265.0. Mean return over all episodes so far = 246.4 \n",
      "Run 273 of 1000 completed with return 259.2. Mean return over all episodes so far = 246.4 \n",
      "Run 274 of 1000 completed with return 235.5. Mean return over all episodes so far = 246.4 \n",
      "Run 275 of 1000 completed with return 245.8. Mean return over all episodes so far = 246.4 \n",
      "Run 276 of 1000 completed with return 252.1. Mean return over all episodes so far = 246.4 \n",
      "Run 277 of 1000 completed with return 264.1. Mean return over all episodes so far = 246.5 \n",
      "Run 278 of 1000 completed with return 247.5. Mean return over all episodes so far = 246.5 \n",
      "Run 279 of 1000 completed with return 258.7. Mean return over all episodes so far = 246.5 \n",
      "Run 280 of 1000 completed with return 261.6. Mean return over all episodes so far = 246.6 \n",
      "Run 281 of 1000 completed with return 275.6. Mean return over all episodes so far = 246.7 \n",
      "Run 282 of 1000 completed with return 252.4. Mean return over all episodes so far = 246.7 \n",
      "Run 283 of 1000 completed with return 269.7. Mean return over all episodes so far = 246.8 \n",
      "Run 284 of 1000 completed with return 255.5. Mean return over all episodes so far = 246.8 \n",
      "Run 285 of 1000 completed with return 254.2. Mean return over all episodes so far = 246.8 \n",
      "Run 286 of 1000 completed with return 251.1. Mean return over all episodes so far = 246.8 \n",
      "Run 287 of 1000 completed with return 291.6. Mean return over all episodes so far = 247.0 \n",
      "Run 288 of 1000 completed with return 271.0. Mean return over all episodes so far = 247.1 \n",
      "Run 289 of 1000 completed with return 280.3. Mean return over all episodes so far = 247.2 \n",
      "Run 290 of 1000 completed with return 268.6. Mean return over all episodes so far = 247.3 \n",
      "Run 291 of 1000 completed with return 221.2. Mean return over all episodes so far = 247.2 \n",
      "Run 292 of 1000 completed with return 266.7. Mean return over all episodes so far = 247.2 \n",
      "Run 293 of 1000 completed with return 120.0. Mean return over all episodes so far = 246.8 \n",
      "Run 294 of 1000 completed with return 278.2. Mean return over all episodes so far = 246.9 \n",
      "Run 295 of 1000 completed with return 219.9. Mean return over all episodes so far = 246.8 \n",
      "Run 296 of 1000 completed with return 217.4. Mean return over all episodes so far = 246.7 \n",
      "Run 297 of 1000 completed with return 286.6. Mean return over all episodes so far = 246.9 \n",
      "Run 298 of 1000 completed with return 115.4. Mean return over all episodes so far = 246.4 \n",
      "Run 299 of 1000 completed with return 245.3. Mean return over all episodes so far = 246.4 \n",
      "Run 300 of 1000 completed with return 252.5. Mean return over all episodes so far = 246.4 \n",
      "Run 301 of 1000 completed with return 231.4. Mean return over all episodes so far = 246.4 \n",
      "Run 302 of 1000 completed with return 232.4. Mean return over all episodes so far = 246.3 \n",
      "Run 303 of 1000 completed with return 262.3. Mean return over all episodes so far = 246.4 \n",
      "Run 304 of 1000 completed with return 264.6. Mean return over all episodes so far = 246.4 \n",
      "Run 305 of 1000 completed with return 242.7. Mean return over all episodes so far = 246.4 \n",
      "Run 306 of 1000 completed with return 245.1. Mean return over all episodes so far = 246.4 \n",
      "Run 307 of 1000 completed with return 265.6. Mean return over all episodes so far = 246.5 \n",
      "Run 308 of 1000 completed with return 255.9. Mean return over all episodes so far = 246.5 \n",
      "Run 309 of 1000 completed with return 228.1. Mean return over all episodes so far = 246.5 \n",
      "Run 310 of 1000 completed with return 208.5. Mean return over all episodes so far = 246.3 \n",
      "Run 311 of 1000 completed with return 260.9. Mean return over all episodes so far = 246.4 \n",
      "Run 312 of 1000 completed with return 261.0. Mean return over all episodes so far = 246.4 \n",
      "Run 313 of 1000 completed with return 263.8. Mean return over all episodes so far = 246.5 \n",
      "Run 314 of 1000 completed with return 259.1. Mean return over all episodes so far = 246.5 \n",
      "Run 315 of 1000 completed with return 245.7. Mean return over all episodes so far = 246.5 \n",
      "Run 316 of 1000 completed with return 215.8. Mean return over all episodes so far = 246.4 \n",
      "Run 317 of 1000 completed with return 252.6. Mean return over all episodes so far = 246.5 \n",
      "Run 318 of 1000 completed with return 205.1. Mean return over all episodes so far = 246.3 \n",
      "Run 319 of 1000 completed with return 227.4. Mean return over all episodes so far = 246.3 \n",
      "Run 320 of 1000 completed with return 247.6. Mean return over all episodes so far = 246.3 \n",
      "Run 321 of 1000 completed with return 205.7. Mean return over all episodes so far = 246.1 \n",
      "Run 322 of 1000 completed with return 243.7. Mean return over all episodes so far = 246.1 \n",
      "Run 323 of 1000 completed with return 236.3. Mean return over all episodes so far = 246.1 \n",
      "Run 324 of 1000 completed with return 233.6. Mean return over all episodes so far = 246.1 \n",
      "Run 325 of 1000 completed with return 219.9. Mean return over all episodes so far = 246.0 \n",
      "Run 326 of 1000 completed with return 231.7. Mean return over all episodes so far = 245.9 \n",
      "Run 327 of 1000 completed with return 256.3. Mean return over all episodes so far = 246.0 \n",
      "Run 328 of 1000 completed with return 249.9. Mean return over all episodes so far = 246.0 \n",
      "Run 329 of 1000 completed with return 258.1. Mean return over all episodes so far = 246.0 \n",
      "Run 330 of 1000 completed with return 249.3. Mean return over all episodes so far = 246.0 \n",
      "Run 331 of 1000 completed with return 239.8. Mean return over all episodes so far = 246.0 \n",
      "Run 332 of 1000 completed with return 239.9. Mean return over all episodes so far = 246.0 \n",
      "Run 333 of 1000 completed with return 274.0. Mean return over all episodes so far = 246.1 \n",
      "Run 334 of 1000 completed with return 231.1. Mean return over all episodes so far = 246.0 \n",
      "Run 335 of 1000 completed with return 156.5. Mean return over all episodes so far = 245.8 \n",
      "Run 336 of 1000 completed with return 242.4. Mean return over all episodes so far = 245.8 \n",
      "Run 337 of 1000 completed with return 271.0. Mean return over all episodes so far = 245.8 \n",
      "Run 338 of 1000 completed with return 268.3. Mean return over all episodes so far = 245.9 \n",
      "Run 339 of 1000 completed with return 240.0. Mean return over all episodes so far = 245.9 \n",
      "Run 340 of 1000 completed with return 261.4. Mean return over all episodes so far = 245.9 \n",
      "Run 341 of 1000 completed with return 266.1. Mean return over all episodes so far = 246.0 \n",
      "Run 342 of 1000 completed with return 275.2. Mean return over all episodes so far = 246.1 \n",
      "Run 343 of 1000 completed with return 265.1. Mean return over all episodes so far = 246.1 \n",
      "Run 344 of 1000 completed with return 258.1. Mean return over all episodes so far = 246.2 \n",
      "Run 345 of 1000 completed with return 206.5. Mean return over all episodes so far = 246.0 \n",
      "Run 346 of 1000 completed with return 270.4. Mean return over all episodes so far = 246.1 \n",
      "Run 347 of 1000 completed with return 219.2. Mean return over all episodes so far = 246.0 \n",
      "Run 348 of 1000 completed with return 251.0. Mean return over all episodes so far = 246.1 \n",
      "Run 349 of 1000 completed with return 247.0. Mean return over all episodes so far = 246.1 \n",
      "Run 350 of 1000 completed with return 230.8. Mean return over all episodes so far = 246.0 \n",
      "Run 351 of 1000 completed with return 276.2. Mean return over all episodes so far = 246.1 \n",
      "Run 352 of 1000 completed with return 287.7. Mean return over all episodes so far = 246.2 \n",
      "Run 353 of 1000 completed with return 277.8. Mean return over all episodes so far = 246.3 \n",
      "Run 354 of 1000 completed with return 241.9. Mean return over all episodes so far = 246.3 \n",
      "Run 355 of 1000 completed with return 267.7. Mean return over all episodes so far = 246.4 \n",
      "Run 356 of 1000 completed with return 253.6. Mean return over all episodes so far = 246.4 \n",
      "Run 357 of 1000 completed with return 266.4. Mean return over all episodes so far = 246.4 \n",
      "Run 358 of 1000 completed with return 138.0. Mean return over all episodes so far = 246.1 \n",
      "Run 359 of 1000 completed with return 269.0. Mean return over all episodes so far = 246.2 \n",
      "Run 360 of 1000 completed with return 158.6. Mean return over all episodes so far = 245.9 \n",
      "Run 361 of 1000 completed with return 214.2. Mean return over all episodes so far = 245.9 \n",
      "Run 362 of 1000 completed with return 283.3. Mean return over all episodes so far = 246.0 \n",
      "Run 363 of 1000 completed with return 262.5. Mean return over all episodes so far = 246.0 \n",
      "Run 364 of 1000 completed with return 238.6. Mean return over all episodes so far = 246.0 \n",
      "Run 365 of 1000 completed with return 250.2. Mean return over all episodes so far = 246.0 \n",
      "Run 366 of 1000 completed with return 247.7. Mean return over all episodes so far = 246.0 \n",
      "Run 367 of 1000 completed with return 250.4. Mean return over all episodes so far = 246.0 \n",
      "Run 368 of 1000 completed with return 243.6. Mean return over all episodes so far = 246.0 \n",
      "Run 369 of 1000 completed with return 225.6. Mean return over all episodes so far = 246.0 \n",
      "Run 370 of 1000 completed with return 270.1. Mean return over all episodes so far = 246.0 \n",
      "Run 371 of 1000 completed with return 220.5. Mean return over all episodes so far = 246.0 \n",
      "Run 372 of 1000 completed with return 250.8. Mean return over all episodes so far = 246.0 \n",
      "Run 373 of 1000 completed with return 216.5. Mean return over all episodes so far = 245.9 \n",
      "Run 374 of 1000 completed with return 249.6. Mean return over all episodes so far = 245.9 \n",
      "Run 375 of 1000 completed with return 269.7. Mean return over all episodes so far = 246.0 \n",
      "Run 376 of 1000 completed with return 274.6. Mean return over all episodes so far = 246.0 \n",
      "Run 377 of 1000 completed with return 242.3. Mean return over all episodes so far = 246.0 \n",
      "Run 378 of 1000 completed with return 259.0. Mean return over all episodes so far = 246.1 \n",
      "Run 379 of 1000 completed with return 240.2. Mean return over all episodes so far = 246.0 \n",
      "Run 380 of 1000 completed with return 256.5. Mean return over all episodes so far = 246.1 \n",
      "Run 381 of 1000 completed with return 264.3. Mean return over all episodes so far = 246.1 \n",
      "Run 382 of 1000 completed with return 105.2. Mean return over all episodes so far = 245.7 \n",
      "Run 383 of 1000 completed with return 238.9. Mean return over all episodes so far = 245.7 \n",
      "Run 384 of 1000 completed with return 251.0. Mean return over all episodes so far = 245.7 \n",
      "Run 385 of 1000 completed with return 263.8. Mean return over all episodes so far = 245.8 \n",
      "Run 386 of 1000 completed with return 245.4. Mean return over all episodes so far = 245.8 \n",
      "Run 387 of 1000 completed with return 261.8. Mean return over all episodes so far = 245.8 \n",
      "Run 388 of 1000 completed with return 285.5. Mean return over all episodes so far = 245.9 \n",
      "Run 389 of 1000 completed with return 253.0. Mean return over all episodes so far = 246.0 \n",
      "Run 390 of 1000 completed with return 257.7. Mean return over all episodes so far = 246.0 \n",
      "Run 391 of 1000 completed with return 265.7. Mean return over all episodes so far = 246.0 \n",
      "Run 392 of 1000 completed with return 248.5. Mean return over all episodes so far = 246.0 \n",
      "Run 393 of 1000 completed with return 263.4. Mean return over all episodes so far = 246.1 \n",
      "Run 394 of 1000 completed with return 263.6. Mean return over all episodes so far = 246.1 \n",
      "Run 395 of 1000 completed with return 259.1. Mean return over all episodes so far = 246.2 \n",
      "Run 396 of 1000 completed with return 251.4. Mean return over all episodes so far = 246.2 \n",
      "Run 397 of 1000 completed with return 259.9. Mean return over all episodes so far = 246.2 \n",
      "Run 398 of 1000 completed with return 255.3. Mean return over all episodes so far = 246.2 \n",
      "Run 399 of 1000 completed with return 241.7. Mean return over all episodes so far = 246.2 \n",
      "Run 400 of 1000 completed with return 234.3. Mean return over all episodes so far = 246.2 \n",
      "Run 401 of 1000 completed with return 291.4. Mean return over all episodes so far = 246.3 \n",
      "Run 402 of 1000 completed with return 277.7. Mean return over all episodes so far = 246.4 \n",
      "Run 403 of 1000 completed with return 241.0. Mean return over all episodes so far = 246.4 \n",
      "Run 404 of 1000 completed with return 250.7. Mean return over all episodes so far = 246.4 \n",
      "Run 405 of 1000 completed with return 259.3. Mean return over all episodes so far = 246.4 \n",
      "Run 406 of 1000 completed with return 215.8. Mean return over all episodes so far = 246.3 \n",
      "Run 407 of 1000 completed with return 269.7. Mean return over all episodes so far = 246.4 \n",
      "Run 408 of 1000 completed with return 259.9. Mean return over all episodes so far = 246.4 \n",
      "Run 409 of 1000 completed with return 256.9. Mean return over all episodes so far = 246.5 \n",
      "Run 410 of 1000 completed with return 269.4. Mean return over all episodes so far = 246.5 \n",
      "Run 411 of 1000 completed with return 247.9. Mean return over all episodes so far = 246.5 \n",
      "Run 412 of 1000 completed with return 246.4. Mean return over all episodes so far = 246.5 \n",
      "Run 413 of 1000 completed with return 250.6. Mean return over all episodes so far = 246.5 \n",
      "Run 414 of 1000 completed with return 255.0. Mean return over all episodes so far = 246.5 \n",
      "Run 415 of 1000 completed with return 220.9. Mean return over all episodes so far = 246.5 \n",
      "Run 416 of 1000 completed with return 259.7. Mean return over all episodes so far = 246.5 \n",
      "Run 417 of 1000 completed with return 259.0. Mean return over all episodes so far = 246.5 \n",
      "Run 418 of 1000 completed with return 244.2. Mean return over all episodes so far = 246.5 \n",
      "Run 419 of 1000 completed with return 256.0. Mean return over all episodes so far = 246.6 \n",
      "Run 420 of 1000 completed with return -46.4. Mean return over all episodes so far = 245.9 \n",
      "Run 421 of 1000 completed with return 257.2. Mean return over all episodes so far = 245.9 \n",
      "Run 422 of 1000 completed with return 262.6. Mean return over all episodes so far = 245.9 \n",
      "Run 423 of 1000 completed with return 254.1. Mean return over all episodes so far = 245.9 \n",
      "Run 424 of 1000 completed with return 244.3. Mean return over all episodes so far = 245.9 \n",
      "Run 425 of 1000 completed with return 252.3. Mean return over all episodes so far = 246.0 \n",
      "Run 426 of 1000 completed with return 228.8. Mean return over all episodes so far = 245.9 \n",
      "Run 427 of 1000 completed with return 241.9. Mean return over all episodes so far = 245.9 \n",
      "Run 428 of 1000 completed with return -95.5. Mean return over all episodes so far = 245.1 \n",
      "Run 429 of 1000 completed with return 259.3. Mean return over all episodes so far = 245.1 \n",
      "Run 430 of 1000 completed with return 204.2. Mean return over all episodes so far = 245.0 \n",
      "Run 431 of 1000 completed with return 241.7. Mean return over all episodes so far = 245.0 \n",
      "Run 432 of 1000 completed with return 260.1. Mean return over all episodes so far = 245.1 \n",
      "Run 433 of 1000 completed with return 299.2. Mean return over all episodes so far = 245.2 \n",
      "Run 434 of 1000 completed with return 262.5. Mean return over all episodes so far = 245.2 \n",
      "Run 435 of 1000 completed with return 264.7. Mean return over all episodes so far = 245.3 \n",
      "Run 436 of 1000 completed with return 236.2. Mean return over all episodes so far = 245.3 \n",
      "Run 437 of 1000 completed with return 258.5. Mean return over all episodes so far = 245.3 \n",
      "Run 438 of 1000 completed with return 248.1. Mean return over all episodes so far = 245.3 \n",
      "Run 439 of 1000 completed with return 256.3. Mean return over all episodes so far = 245.3 \n",
      "Run 440 of 1000 completed with return 273.1. Mean return over all episodes so far = 245.4 \n",
      "Run 441 of 1000 completed with return 251.3. Mean return over all episodes so far = 245.4 \n",
      "Run 442 of 1000 completed with return 256.4. Mean return over all episodes so far = 245.4 \n",
      "Run 443 of 1000 completed with return 252.6. Mean return over all episodes so far = 245.4 \n",
      "Run 444 of 1000 completed with return 160.3. Mean return over all episodes so far = 245.3 \n",
      "Run 445 of 1000 completed with return 267.9. Mean return over all episodes so far = 245.3 \n",
      "Run 446 of 1000 completed with return 259.4. Mean return over all episodes so far = 245.3 \n",
      "Run 447 of 1000 completed with return 245.3. Mean return over all episodes so far = 245.3 \n",
      "Run 448 of 1000 completed with return 254.9. Mean return over all episodes so far = 245.4 \n",
      "Run 449 of 1000 completed with return 244.1. Mean return over all episodes so far = 245.4 \n",
      "Run 450 of 1000 completed with return 268.2. Mean return over all episodes so far = 245.4 \n",
      "Run 451 of 1000 completed with return 268.4. Mean return over all episodes so far = 245.5 \n",
      "Run 452 of 1000 completed with return 232.2. Mean return over all episodes so far = 245.4 \n",
      "Run 453 of 1000 completed with return 201.5. Mean return over all episodes so far = 245.3 \n",
      "Run 454 of 1000 completed with return 258.7. Mean return over all episodes so far = 245.4 \n",
      "Run 455 of 1000 completed with return 117.8. Mean return over all episodes so far = 245.1 \n",
      "Run 456 of 1000 completed with return 267.8. Mean return over all episodes so far = 245.1 \n",
      "Run 457 of 1000 completed with return 254.5. Mean return over all episodes so far = 245.1 \n",
      "Run 458 of 1000 completed with return 242.0. Mean return over all episodes so far = 245.1 \n",
      "Run 459 of 1000 completed with return 260.0. Mean return over all episodes so far = 245.2 \n",
      "Run 460 of 1000 completed with return 266.5. Mean return over all episodes so far = 245.2 \n",
      "Run 461 of 1000 completed with return 247.7. Mean return over all episodes so far = 245.2 \n",
      "Run 462 of 1000 completed with return 251.1. Mean return over all episodes so far = 245.2 \n",
      "Run 463 of 1000 completed with return 251.8. Mean return over all episodes so far = 245.3 \n",
      "Run 464 of 1000 completed with return 258.0. Mean return over all episodes so far = 245.3 \n",
      "Run 465 of 1000 completed with return 270.7. Mean return over all episodes so far = 245.3 \n",
      "Run 466 of 1000 completed with return 240.2. Mean return over all episodes so far = 245.3 \n",
      "Run 467 of 1000 completed with return 252.3. Mean return over all episodes so far = 245.3 \n",
      "Run 468 of 1000 completed with return 273.1. Mean return over all episodes so far = 245.4 \n",
      "Run 469 of 1000 completed with return 230.7. Mean return over all episodes so far = 245.4 \n",
      "Run 470 of 1000 completed with return 253.7. Mean return over all episodes so far = 245.4 \n",
      "Run 471 of 1000 completed with return 281.4. Mean return over all episodes so far = 245.5 \n",
      "Run 472 of 1000 completed with return 252.5. Mean return over all episodes so far = 245.5 \n",
      "Run 473 of 1000 completed with return 104.6. Mean return over all episodes so far = 245.2 \n",
      "Run 474 of 1000 completed with return 243.5. Mean return over all episodes so far = 245.2 \n",
      "Run 475 of 1000 completed with return 268.9. Mean return over all episodes so far = 245.2 \n",
      "Run 476 of 1000 completed with return 259.0. Mean return over all episodes so far = 245.3 \n",
      "Run 477 of 1000 completed with return 227.0. Mean return over all episodes so far = 245.2 \n",
      "Run 478 of 1000 completed with return 258.3. Mean return over all episodes so far = 245.2 \n",
      "Run 479 of 1000 completed with return 253.7. Mean return over all episodes so far = 245.3 \n",
      "Run 480 of 1000 completed with return 242.2. Mean return over all episodes so far = 245.3 \n",
      "Run 481 of 1000 completed with return 258.6. Mean return over all episodes so far = 245.3 \n",
      "Run 482 of 1000 completed with return 121.5. Mean return over all episodes so far = 245.0 \n",
      "Run 483 of 1000 completed with return 262.9. Mean return over all episodes so far = 245.1 \n",
      "Run 484 of 1000 completed with return 213.3. Mean return over all episodes so far = 245.0 \n",
      "Run 485 of 1000 completed with return 253.2. Mean return over all episodes so far = 245.0 \n",
      "Run 486 of 1000 completed with return 229.1. Mean return over all episodes so far = 245.0 \n",
      "Run 487 of 1000 completed with return 271.7. Mean return over all episodes so far = 245.0 \n",
      "Run 488 of 1000 completed with return 268.0. Mean return over all episodes so far = 245.1 \n",
      "Run 489 of 1000 completed with return 257.2. Mean return over all episodes so far = 245.1 \n",
      "Run 490 of 1000 completed with return 253.4. Mean return over all episodes so far = 245.1 \n",
      "Run 491 of 1000 completed with return 286.5. Mean return over all episodes so far = 245.2 \n",
      "Run 492 of 1000 completed with return 248.0. Mean return over all episodes so far = 245.2 \n",
      "Run 493 of 1000 completed with return 253.3. Mean return over all episodes so far = 245.2 \n",
      "Run 494 of 1000 completed with return 262.4. Mean return over all episodes so far = 245.3 \n",
      "Run 495 of 1000 completed with return 260.5. Mean return over all episodes so far = 245.3 \n",
      "Run 496 of 1000 completed with return 265.5. Mean return over all episodes so far = 245.3 \n",
      "Run 497 of 1000 completed with return 243.2. Mean return over all episodes so far = 245.3 \n",
      "Run 498 of 1000 completed with return 256.2. Mean return over all episodes so far = 245.4 \n",
      "Run 499 of 1000 completed with return 210.4. Mean return over all episodes so far = 245.3 \n",
      "Run 500 of 1000 completed with return 253.8. Mean return over all episodes so far = 245.3 \n",
      "Run 501 of 1000 completed with return 258.9. Mean return over all episodes so far = 245.3 \n",
      "Run 502 of 1000 completed with return 124.1. Mean return over all episodes so far = 245.1 \n",
      "Run 503 of 1000 completed with return 267.7. Mean return over all episodes so far = 245.1 \n",
      "Run 504 of 1000 completed with return 250.7. Mean return over all episodes so far = 245.1 \n",
      "Run 505 of 1000 completed with return 233.7. Mean return over all episodes so far = 245.1 \n",
      "Run 506 of 1000 completed with return 257.7. Mean return over all episodes so far = 245.1 \n",
      "Run 507 of 1000 completed with return 256.0. Mean return over all episodes so far = 245.2 \n",
      "Run 508 of 1000 completed with return 284.7. Mean return over all episodes so far = 245.2 \n",
      "Run 509 of 1000 completed with return 237.6. Mean return over all episodes so far = 245.2 \n",
      "Run 510 of 1000 completed with return 230.1. Mean return over all episodes so far = 245.2 \n",
      "Run 511 of 1000 completed with return 149.1. Mean return over all episodes so far = 245.0 \n",
      "Run 512 of 1000 completed with return 264.0. Mean return over all episodes so far = 245.0 \n",
      "Run 513 of 1000 completed with return 214.4. Mean return over all episodes so far = 245.0 \n",
      "Run 514 of 1000 completed with return 255.1. Mean return over all episodes so far = 245.0 \n",
      "Run 515 of 1000 completed with return 251.1. Mean return over all episodes so far = 245.0 \n",
      "Run 516 of 1000 completed with return 269.8. Mean return over all episodes so far = 245.1 \n",
      "Run 517 of 1000 completed with return 265.0. Mean return over all episodes so far = 245.1 \n",
      "Run 518 of 1000 completed with return 253.2. Mean return over all episodes so far = 245.1 \n",
      "Run 519 of 1000 completed with return 224.2. Mean return over all episodes so far = 245.1 \n",
      "Run 520 of 1000 completed with return 250.6. Mean return over all episodes so far = 245.1 \n",
      "Run 521 of 1000 completed with return 212.1. Mean return over all episodes so far = 245.0 \n",
      "Run 522 of 1000 completed with return 275.0. Mean return over all episodes so far = 245.1 \n",
      "Run 523 of 1000 completed with return 268.6. Mean return over all episodes so far = 245.1 \n",
      "Run 524 of 1000 completed with return -99.9. Mean return over all episodes so far = 244.5 \n",
      "Run 525 of 1000 completed with return 266.9. Mean return over all episodes so far = 244.5 \n",
      "Run 526 of 1000 completed with return 250.3. Mean return over all episodes so far = 244.5 \n",
      "Run 527 of 1000 completed with return 264.0. Mean return over all episodes so far = 244.6 \n",
      "Run 528 of 1000 completed with return 231.2. Mean return over all episodes so far = 244.5 \n",
      "Run 529 of 1000 completed with return 251.0. Mean return over all episodes so far = 244.5 \n",
      "Run 530 of 1000 completed with return 276.4. Mean return over all episodes so far = 244.6 \n",
      "Run 531 of 1000 completed with return 265.4. Mean return over all episodes so far = 244.6 \n",
      "Run 532 of 1000 completed with return 225.7. Mean return over all episodes so far = 244.6 \n",
      "Run 533 of 1000 completed with return 250.0. Mean return over all episodes so far = 244.6 \n",
      "Run 534 of 1000 completed with return 269.4. Mean return over all episodes so far = 244.7 \n",
      "Run 535 of 1000 completed with return 255.5. Mean return over all episodes so far = 244.7 \n",
      "Run 536 of 1000 completed with return 270.6. Mean return over all episodes so far = 244.7 \n",
      "Run 537 of 1000 completed with return 281.4. Mean return over all episodes so far = 244.8 \n",
      "Run 538 of 1000 completed with return 272.0. Mean return over all episodes so far = 244.9 \n",
      "Run 539 of 1000 completed with return 213.1. Mean return over all episodes so far = 244.8 \n",
      "Run 540 of 1000 completed with return 246.4. Mean return over all episodes so far = 244.8 \n",
      "Run 541 of 1000 completed with return 259.4. Mean return over all episodes so far = 244.8 \n",
      "Run 542 of 1000 completed with return 238.4. Mean return over all episodes so far = 244.8 \n",
      "Run 543 of 1000 completed with return 246.2. Mean return over all episodes so far = 244.8 \n",
      "Run 544 of 1000 completed with return 124.5. Mean return over all episodes so far = 244.6 \n",
      "Run 545 of 1000 completed with return 252.6. Mean return over all episodes so far = 244.6 \n",
      "Run 546 of 1000 completed with return 272.2. Mean return over all episodes so far = 244.7 \n",
      "Run 547 of 1000 completed with return 241.5. Mean return over all episodes so far = 244.7 \n",
      "Run 548 of 1000 completed with return 262.4. Mean return over all episodes so far = 244.7 \n",
      "Run 549 of 1000 completed with return 267.3. Mean return over all episodes so far = 244.7 \n",
      "Run 550 of 1000 completed with return 235.6. Mean return over all episodes so far = 244.7 \n",
      "Run 551 of 1000 completed with return 208.9. Mean return over all episodes so far = 244.6 \n",
      "Run 552 of 1000 completed with return 261.1. Mean return over all episodes so far = 244.7 \n",
      "Run 553 of 1000 completed with return 210.9. Mean return over all episodes so far = 244.6 \n",
      "Run 554 of 1000 completed with return 266.0. Mean return over all episodes so far = 244.7 \n",
      "Run 555 of 1000 completed with return 203.9. Mean return over all episodes so far = 244.6 \n",
      "Run 556 of 1000 completed with return 257.2. Mean return over all episodes so far = 244.6 \n",
      "Run 557 of 1000 completed with return 289.7. Mean return over all episodes so far = 244.7 \n",
      "Run 558 of 1000 completed with return 259.3. Mean return over all episodes so far = 244.7 \n",
      "Run 559 of 1000 completed with return 261.7. Mean return over all episodes so far = 244.7 \n",
      "Run 560 of 1000 completed with return 258.8. Mean return over all episodes so far = 244.8 \n",
      "Run 561 of 1000 completed with return 255.7. Mean return over all episodes so far = 244.8 \n",
      "Run 562 of 1000 completed with return 238.3. Mean return over all episodes so far = 244.8 \n",
      "Run 563 of 1000 completed with return 258.0. Mean return over all episodes so far = 244.8 \n",
      "Run 564 of 1000 completed with return 232.6. Mean return over all episodes so far = 244.8 \n",
      "Run 565 of 1000 completed with return 292.7. Mean return over all episodes so far = 244.9 \n",
      "Run 566 of 1000 completed with return 223.2. Mean return over all episodes so far = 244.8 \n",
      "Run 567 of 1000 completed with return 240.1. Mean return over all episodes so far = 244.8 \n",
      "Run 568 of 1000 completed with return 262.6. Mean return over all episodes so far = 244.8 \n",
      "Run 569 of 1000 completed with return 239.7. Mean return over all episodes so far = 244.8 \n",
      "Run 570 of 1000 completed with return 224.6. Mean return over all episodes so far = 244.8 \n",
      "Run 571 of 1000 completed with return 253.0. Mean return over all episodes so far = 244.8 \n",
      "Run 572 of 1000 completed with return 243.3. Mean return over all episodes so far = 244.8 \n",
      "Run 573 of 1000 completed with return 240.0. Mean return over all episodes so far = 244.8 \n",
      "Run 574 of 1000 completed with return 261.5. Mean return over all episodes so far = 244.8 \n",
      "Run 575 of 1000 completed with return 247.4. Mean return over all episodes so far = 244.8 \n",
      "Run 576 of 1000 completed with return 246.8. Mean return over all episodes so far = 244.8 \n",
      "Run 577 of 1000 completed with return 239.0. Mean return over all episodes so far = 244.8 \n",
      "Run 578 of 1000 completed with return 256.2. Mean return over all episodes so far = 244.9 \n",
      "Run 579 of 1000 completed with return 262.9. Mean return over all episodes so far = 244.9 \n",
      "Run 580 of 1000 completed with return 251.0. Mean return over all episodes so far = 244.9 \n",
      "Run 581 of 1000 completed with return 271.0. Mean return over all episodes so far = 244.9 \n",
      "Run 582 of 1000 completed with return 290.5. Mean return over all episodes so far = 245.0 \n",
      "Run 583 of 1000 completed with return 234.5. Mean return over all episodes so far = 245.0 \n",
      "Run 584 of 1000 completed with return 261.2. Mean return over all episodes so far = 245.0 \n",
      "Run 585 of 1000 completed with return 256.8. Mean return over all episodes so far = 245.0 \n",
      "Run 586 of 1000 completed with return 241.0. Mean return over all episodes so far = 245.0 \n",
      "Run 587 of 1000 completed with return 282.5. Mean return over all episodes so far = 245.1 \n",
      "Run 588 of 1000 completed with return 253.5. Mean return over all episodes so far = 245.1 \n",
      "Run 589 of 1000 completed with return 273.9. Mean return over all episodes so far = 245.2 \n",
      "Run 590 of 1000 completed with return 235.1. Mean return over all episodes so far = 245.1 \n",
      "Run 591 of 1000 completed with return 267.8. Mean return over all episodes so far = 245.2 \n",
      "Run 592 of 1000 completed with return 242.7. Mean return over all episodes so far = 245.2 \n",
      "Run 593 of 1000 completed with return 244.4. Mean return over all episodes so far = 245.2 \n",
      "Run 594 of 1000 completed with return 255.9. Mean return over all episodes so far = 245.2 \n",
      "Run 595 of 1000 completed with return 254.5. Mean return over all episodes so far = 245.2 \n",
      "Run 596 of 1000 completed with return 246.9. Mean return over all episodes so far = 245.2 \n",
      "Run 597 of 1000 completed with return 267.3. Mean return over all episodes so far = 245.3 \n",
      "Run 598 of 1000 completed with return 237.0. Mean return over all episodes so far = 245.2 \n",
      "Run 599 of 1000 completed with return 226.2. Mean return over all episodes so far = 245.2 \n",
      "Run 600 of 1000 completed with return 248.2. Mean return over all episodes so far = 245.2 \n",
      "Run 601 of 1000 completed with return 266.4. Mean return over all episodes so far = 245.3 \n",
      "Run 602 of 1000 completed with return 248.7. Mean return over all episodes so far = 245.3 \n",
      "Run 603 of 1000 completed with return 245.3. Mean return over all episodes so far = 245.3 \n",
      "Run 604 of 1000 completed with return 256.4. Mean return over all episodes so far = 245.3 \n",
      "Run 605 of 1000 completed with return 245.6. Mean return over all episodes so far = 245.3 \n",
      "Run 606 of 1000 completed with return 266.7. Mean return over all episodes so far = 245.3 \n",
      "Run 607 of 1000 completed with return 246.7. Mean return over all episodes so far = 245.3 \n",
      "Run 608 of 1000 completed with return 252.2. Mean return over all episodes so far = 245.3 \n",
      "Run 609 of 1000 completed with return 209.2. Mean return over all episodes so far = 245.3 \n",
      "Run 610 of 1000 completed with return 256.3. Mean return over all episodes so far = 245.3 \n",
      "Run 611 of 1000 completed with return 256.5. Mean return over all episodes so far = 245.3 \n",
      "Run 612 of 1000 completed with return 246.2. Mean return over all episodes so far = 245.3 \n",
      "Run 613 of 1000 completed with return 242.5. Mean return over all episodes so far = 245.3 \n",
      "Run 614 of 1000 completed with return 265.1. Mean return over all episodes so far = 245.3 \n",
      "Run 615 of 1000 completed with return 257.0. Mean return over all episodes so far = 245.3 \n",
      "Run 616 of 1000 completed with return 262.4. Mean return over all episodes so far = 245.4 \n",
      "Run 617 of 1000 completed with return 238.8. Mean return over all episodes so far = 245.4 \n",
      "Run 618 of 1000 completed with return 257.6. Mean return over all episodes so far = 245.4 \n",
      "Run 619 of 1000 completed with return 254.3. Mean return over all episodes so far = 245.4 \n",
      "Run 620 of 1000 completed with return 237.6. Mean return over all episodes so far = 245.4 \n",
      "Run 621 of 1000 completed with return 253.1. Mean return over all episodes so far = 245.4 \n",
      "Run 622 of 1000 completed with return 276.2. Mean return over all episodes so far = 245.4 \n",
      "Run 623 of 1000 completed with return 246.9. Mean return over all episodes so far = 245.5 \n",
      "Run 624 of 1000 completed with return 230.6. Mean return over all episodes so far = 245.4 \n",
      "Run 625 of 1000 completed with return 246.8. Mean return over all episodes so far = 245.4 \n",
      "Run 626 of 1000 completed with return 250.6. Mean return over all episodes so far = 245.4 \n",
      "Run 627 of 1000 completed with return 282.0. Mean return over all episodes so far = 245.5 \n",
      "Run 628 of 1000 completed with return 244.1. Mean return over all episodes so far = 245.5 \n",
      "Run 629 of 1000 completed with return 274.2. Mean return over all episodes so far = 245.5 \n",
      "Run 630 of 1000 completed with return 264.2. Mean return over all episodes so far = 245.6 \n",
      "Run 631 of 1000 completed with return 19.8 . Mean return over all episodes so far = 245.2 \n",
      "Run 632 of 1000 completed with return 266.8. Mean return over all episodes so far = 245.2 \n",
      "Run 633 of 1000 completed with return 266.5. Mean return over all episodes so far = 245.3 \n",
      "Run 634 of 1000 completed with return 225.8. Mean return over all episodes so far = 245.2 \n",
      "Run 635 of 1000 completed with return 262.2. Mean return over all episodes so far = 245.3 \n",
      "Run 636 of 1000 completed with return 240.8. Mean return over all episodes so far = 245.3 \n",
      "Run 637 of 1000 completed with return 249.8. Mean return over all episodes so far = 245.3 \n",
      "Run 638 of 1000 completed with return 251.1. Mean return over all episodes so far = 245.3 \n",
      "Run 639 of 1000 completed with return 273.4. Mean return over all episodes so far = 245.3 \n",
      "Run 640 of 1000 completed with return 240.3. Mean return over all episodes so far = 245.3 \n",
      "Run 641 of 1000 completed with return 114.8. Mean return over all episodes so far = 245.1 \n",
      "Run 642 of 1000 completed with return 217.2. Mean return over all episodes so far = 245.1 \n",
      "Run 643 of 1000 completed with return -98.6. Mean return over all episodes so far = 244.5 \n",
      "Run 644 of 1000 completed with return 123.1. Mean return over all episodes so far = 244.4 \n",
      "Run 645 of 1000 completed with return 245.2. Mean return over all episodes so far = 244.4 \n",
      "Run 646 of 1000 completed with return 236.3. Mean return over all episodes so far = 244.3 \n",
      "Run 647 of 1000 completed with return 236.6. Mean return over all episodes so far = 244.3 \n",
      "Run 648 of 1000 completed with return 266.9. Mean return over all episodes so far = 244.4 \n",
      "Run 649 of 1000 completed with return 265.6. Mean return over all episodes so far = 244.4 \n",
      "Run 650 of 1000 completed with return 260.5. Mean return over all episodes so far = 244.4 \n",
      "Run 651 of 1000 completed with return 274.8. Mean return over all episodes so far = 244.5 \n",
      "Run 652 of 1000 completed with return 289.6. Mean return over all episodes so far = 244.5 \n",
      "Run 653 of 1000 completed with return 147.9. Mean return over all episodes so far = 244.4 \n",
      "Run 654 of 1000 completed with return 292.7. Mean return over all episodes so far = 244.5 \n",
      "Run 655 of 1000 completed with return 261.2. Mean return over all episodes so far = 244.5 \n",
      "Run 656 of 1000 completed with return 259.6. Mean return over all episodes so far = 244.5 \n",
      "Run 657 of 1000 completed with return 237.3. Mean return over all episodes so far = 244.5 \n",
      "Run 658 of 1000 completed with return 278.8. Mean return over all episodes so far = 244.6 \n",
      "Run 659 of 1000 completed with return 130.4. Mean return over all episodes so far = 244.4 \n",
      "Run 660 of 1000 completed with return 246.1. Mean return over all episodes so far = 244.4 \n",
      "Run 661 of 1000 completed with return 288.8. Mean return over all episodes so far = 244.4 \n",
      "Run 662 of 1000 completed with return 272.5. Mean return over all episodes so far = 244.5 \n",
      "Run 663 of 1000 completed with return 261.4. Mean return over all episodes so far = 244.5 \n",
      "Run 664 of 1000 completed with return 251.7. Mean return over all episodes so far = 244.5 \n",
      "Run 665 of 1000 completed with return -49.8. Mean return over all episodes so far = 244.1 \n",
      "Run 666 of 1000 completed with return 262.6. Mean return over all episodes so far = 244.1 \n",
      "Run 667 of 1000 completed with return 268.5. Mean return over all episodes so far = 244.1 \n",
      "Run 668 of 1000 completed with return 254.7. Mean return over all episodes so far = 244.2 \n",
      "Run 669 of 1000 completed with return 254.2. Mean return over all episodes so far = 244.2 \n",
      "Run 670 of 1000 completed with return 271.4. Mean return over all episodes so far = 244.2 \n",
      "Run 671 of 1000 completed with return 244.8. Mean return over all episodes so far = 244.2 \n",
      "Run 672 of 1000 completed with return 264.7. Mean return over all episodes so far = 244.3 \n",
      "Run 673 of 1000 completed with return 249.1. Mean return over all episodes so far = 244.3 \n",
      "Run 674 of 1000 completed with return 125.1. Mean return over all episodes so far = 244.1 \n",
      "Run 675 of 1000 completed with return 149.3. Mean return over all episodes so far = 243.9 \n",
      "Run 676 of 1000 completed with return 255.0. Mean return over all episodes so far = 244.0 \n",
      "Run 677 of 1000 completed with return 270.9. Mean return over all episodes so far = 244.0 \n",
      "Run 678 of 1000 completed with return 256.9. Mean return over all episodes so far = 244.0 \n",
      "Run 679 of 1000 completed with return 249.0. Mean return over all episodes so far = 244.0 \n",
      "Run 680 of 1000 completed with return 254.8. Mean return over all episodes so far = 244.0 \n",
      "Run 681 of 1000 completed with return 244.4. Mean return over all episodes so far = 244.0 \n",
      "Run 682 of 1000 completed with return 249.6. Mean return over all episodes so far = 244.0 \n",
      "Run 683 of 1000 completed with return 246.9. Mean return over all episodes so far = 244.1 \n",
      "Run 684 of 1000 completed with return 132.7. Mean return over all episodes so far = 243.9 \n",
      "Run 685 of 1000 completed with return -19.2. Mean return over all episodes so far = 243.5 \n",
      "Run 686 of 1000 completed with return 221.8. Mean return over all episodes so far = 243.5 \n",
      "Run 687 of 1000 completed with return 263.0. Mean return over all episodes so far = 243.5 \n",
      "Run 688 of 1000 completed with return 266.5. Mean return over all episodes so far = 243.5 \n",
      "Run 689 of 1000 completed with return 254.1. Mean return over all episodes so far = 243.6 \n",
      "Run 690 of 1000 completed with return 281.0. Mean return over all episodes so far = 243.6 \n",
      "Run 691 of 1000 completed with return 291.6. Mean return over all episodes so far = 243.7 \n",
      "Run 692 of 1000 completed with return 267.0. Mean return over all episodes so far = 243.7 \n",
      "Run 693 of 1000 completed with return 229.4. Mean return over all episodes so far = 243.7 \n",
      "Run 694 of 1000 completed with return 266.9. Mean return over all episodes so far = 243.7 \n",
      "Run 695 of 1000 completed with return 227.7. Mean return over all episodes so far = 243.7 \n",
      "Run 696 of 1000 completed with return 248.8. Mean return over all episodes so far = 243.7 \n",
      "Run 697 of 1000 completed with return 240.4. Mean return over all episodes so far = 243.7 \n",
      "Run 698 of 1000 completed with return 269.5. Mean return over all episodes so far = 243.7 \n",
      "Run 699 of 1000 completed with return 274.9. Mean return over all episodes so far = 243.8 \n",
      "Run 700 of 1000 completed with return 133.6. Mean return over all episodes so far = 243.6 \n",
      "Run 701 of 1000 completed with return 267.4. Mean return over all episodes so far = 243.7 \n",
      "Run 702 of 1000 completed with return 234.5. Mean return over all episodes so far = 243.6 \n",
      "Run 703 of 1000 completed with return 235.5. Mean return over all episodes so far = 243.6 \n",
      "Run 704 of 1000 completed with return 245.9. Mean return over all episodes so far = 243.6 \n",
      "Run 705 of 1000 completed with return 263.0. Mean return over all episodes so far = 243.7 \n",
      "Run 706 of 1000 completed with return 234.6. Mean return over all episodes so far = 243.7 \n",
      "Run 707 of 1000 completed with return 244.1. Mean return over all episodes so far = 243.7 \n",
      "Run 708 of 1000 completed with return 262.9. Mean return over all episodes so far = 243.7 \n",
      "Run 709 of 1000 completed with return 136.9. Mean return over all episodes so far = 243.5 \n",
      "Run 710 of 1000 completed with return 248.5. Mean return over all episodes so far = 243.5 \n",
      "Run 711 of 1000 completed with return 263.4. Mean return over all episodes so far = 243.6 \n",
      "Run 712 of 1000 completed with return 266.7. Mean return over all episodes so far = 243.6 \n",
      "Run 713 of 1000 completed with return 125.9. Mean return over all episodes so far = 243.4 \n",
      "Run 714 of 1000 completed with return 261.4. Mean return over all episodes so far = 243.5 \n",
      "Run 715 of 1000 completed with return 278.2. Mean return over all episodes so far = 243.5 \n",
      "Run 716 of 1000 completed with return 234.0. Mean return over all episodes so far = 243.5 \n",
      "Run 717 of 1000 completed with return 252.6. Mean return over all episodes so far = 243.5 \n",
      "Run 718 of 1000 completed with return 245.3. Mean return over all episodes so far = 243.5 \n",
      "Run 719 of 1000 completed with return 235.1. Mean return over all episodes so far = 243.5 \n",
      "Run 720 of 1000 completed with return 271.0. Mean return over all episodes so far = 243.5 \n",
      "Run 721 of 1000 completed with return 149.1. Mean return over all episodes so far = 243.4 \n",
      "Run 722 of 1000 completed with return 266.3. Mean return over all episodes so far = 243.4 \n",
      "Run 723 of 1000 completed with return 249.9. Mean return over all episodes so far = 243.4 \n",
      "Run 724 of 1000 completed with return 264.7. Mean return over all episodes so far = 243.5 \n",
      "Run 725 of 1000 completed with return 205.7. Mean return over all episodes so far = 243.4 \n",
      "Run 726 of 1000 completed with return 233.8. Mean return over all episodes so far = 243.4 \n",
      "Run 727 of 1000 completed with return 268.3. Mean return over all episodes so far = 243.4 \n",
      "Run 728 of 1000 completed with return 266.2. Mean return over all episodes so far = 243.5 \n",
      "Run 729 of 1000 completed with return 228.5. Mean return over all episodes so far = 243.5 \n",
      "Run 730 of 1000 completed with return 255.5. Mean return over all episodes so far = 243.5 \n",
      "Run 731 of 1000 completed with return 255.0. Mean return over all episodes so far = 243.5 \n",
      "Run 732 of 1000 completed with return 280.1. Mean return over all episodes so far = 243.5 \n",
      "Run 733 of 1000 completed with return 235.9. Mean return over all episodes so far = 243.5 \n",
      "Run 734 of 1000 completed with return 261.8. Mean return over all episodes so far = 243.5 \n",
      "Run 735 of 1000 completed with return 149.3. Mean return over all episodes so far = 243.4 \n",
      "Run 736 of 1000 completed with return 232.2. Mean return over all episodes so far = 243.4 \n",
      "Run 737 of 1000 completed with return 253.2. Mean return over all episodes so far = 243.4 \n",
      "Run 738 of 1000 completed with return 253.0. Mean return over all episodes so far = 243.4 \n",
      "Run 739 of 1000 completed with return 284.1. Mean return over all episodes so far = 243.5 \n",
      "Run 740 of 1000 completed with return 257.1. Mean return over all episodes so far = 243.5 \n",
      "Run 741 of 1000 completed with return 245.1. Mean return over all episodes so far = 243.5 \n",
      "Run 742 of 1000 completed with return 264.9. Mean return over all episodes so far = 243.5 \n",
      "Run 743 of 1000 completed with return 249.7. Mean return over all episodes so far = 243.5 \n",
      "Run 744 of 1000 completed with return 223.2. Mean return over all episodes so far = 243.5 \n",
      "Run 745 of 1000 completed with return 117.8. Mean return over all episodes so far = 243.3 \n",
      "Run 746 of 1000 completed with return 296.6. Mean return over all episodes so far = 243.4 \n",
      "Run 747 of 1000 completed with return 243.1. Mean return over all episodes so far = 243.4 \n",
      "Run 748 of 1000 completed with return 270.9. Mean return over all episodes so far = 243.5 \n",
      "Run 749 of 1000 completed with return 257.0. Mean return over all episodes so far = 243.5 \n",
      "Run 750 of 1000 completed with return 277.7. Mean return over all episodes so far = 243.5 \n",
      "Run 751 of 1000 completed with return 245.7. Mean return over all episodes so far = 243.5 \n",
      "Run 752 of 1000 completed with return 271.4. Mean return over all episodes so far = 243.6 \n",
      "Run 753 of 1000 completed with return 170.8. Mean return over all episodes so far = 243.5 \n",
      "Run 754 of 1000 completed with return 227.7. Mean return over all episodes so far = 243.4 \n",
      "Run 755 of 1000 completed with return 244.9. Mean return over all episodes so far = 243.4 \n",
      "Run 756 of 1000 completed with return 273.3. Mean return over all episodes so far = 243.5 \n",
      "Run 757 of 1000 completed with return 259.4. Mean return over all episodes so far = 243.5 \n",
      "Run 758 of 1000 completed with return 274.7. Mean return over all episodes so far = 243.5 \n",
      "Run 759 of 1000 completed with return 255.0. Mean return over all episodes so far = 243.6 \n",
      "Run 760 of 1000 completed with return 237.1. Mean return over all episodes so far = 243.6 \n",
      "Run 761 of 1000 completed with return 265.5. Mean return over all episodes so far = 243.6 \n",
      "Run 762 of 1000 completed with return 98.0 . Mean return over all episodes so far = 243.4 \n",
      "Run 763 of 1000 completed with return 263.5. Mean return over all episodes so far = 243.4 \n",
      "Run 764 of 1000 completed with return 283.9. Mean return over all episodes so far = 243.5 \n",
      "Run 765 of 1000 completed with return 238.8. Mean return over all episodes so far = 243.5 \n",
      "Run 766 of 1000 completed with return 266.6. Mean return over all episodes so far = 243.5 \n",
      "Run 767 of 1000 completed with return 272.6. Mean return over all episodes so far = 243.5 \n",
      "Run 768 of 1000 completed with return 224.6. Mean return over all episodes so far = 243.5 \n",
      "Run 769 of 1000 completed with return 294.4. Mean return over all episodes so far = 243.6 \n",
      "Run 770 of 1000 completed with return 251.9. Mean return over all episodes so far = 243.6 \n",
      "Run 771 of 1000 completed with return 148.6. Mean return over all episodes so far = 243.5 \n",
      "Run 772 of 1000 completed with return 233.6. Mean return over all episodes so far = 243.4 \n",
      "Run 773 of 1000 completed with return 278.2. Mean return over all episodes so far = 243.5 \n",
      "Run 774 of 1000 completed with return 237.3. Mean return over all episodes so far = 243.5 \n",
      "Run 775 of 1000 completed with return 260.5. Mean return over all episodes so far = 243.5 \n",
      "Run 776 of 1000 completed with return 282.8. Mean return over all episodes so far = 243.6 \n",
      "Run 777 of 1000 completed with return 262.4. Mean return over all episodes so far = 243.6 \n",
      "Run 778 of 1000 completed with return 258.9. Mean return over all episodes so far = 243.6 \n",
      "Run 779 of 1000 completed with return 252.2. Mean return over all episodes so far = 243.6 \n",
      "Run 780 of 1000 completed with return 278.6. Mean return over all episodes so far = 243.7 \n",
      "Run 781 of 1000 completed with return 246.5. Mean return over all episodes so far = 243.7 \n",
      "Run 782 of 1000 completed with return 243.1. Mean return over all episodes so far = 243.7 \n",
      "Run 783 of 1000 completed with return 254.9. Mean return over all episodes so far = 243.7 \n",
      "Run 784 of 1000 completed with return 250.3. Mean return over all episodes so far = 243.7 \n",
      "Run 785 of 1000 completed with return 282.4. Mean return over all episodes so far = 243.7 \n",
      "Run 786 of 1000 completed with return 273.0. Mean return over all episodes so far = 243.8 \n",
      "Run 787 of 1000 completed with return 229.6. Mean return over all episodes so far = 243.8 \n",
      "Run 788 of 1000 completed with return 268.7. Mean return over all episodes so far = 243.8 \n",
      "Run 789 of 1000 completed with return 116.0. Mean return over all episodes so far = 243.6 \n",
      "Run 790 of 1000 completed with return 268.9. Mean return over all episodes so far = 243.7 \n",
      "Run 791 of 1000 completed with return 232.8. Mean return over all episodes so far = 243.6 \n",
      "Run 792 of 1000 completed with return 273.3. Mean return over all episodes so far = 243.7 \n",
      "Run 793 of 1000 completed with return 244.3. Mean return over all episodes so far = 243.7 \n",
      "Run 794 of 1000 completed with return 256.3. Mean return over all episodes so far = 243.7 \n",
      "Run 795 of 1000 completed with return 255.6. Mean return over all episodes so far = 243.7 \n",
      "Run 796 of 1000 completed with return 281.3. Mean return over all episodes so far = 243.8 \n",
      "Run 797 of 1000 completed with return 262.1. Mean return over all episodes so far = 243.8 \n",
      "Run 798 of 1000 completed with return 244.4. Mean return over all episodes so far = 243.8 \n",
      "Run 799 of 1000 completed with return 235.4. Mean return over all episodes so far = 243.8 \n",
      "Run 800 of 1000 completed with return 258.0. Mean return over all episodes so far = 243.8 \n",
      "Run 801 of 1000 completed with return 231.6. Mean return over all episodes so far = 243.8 \n",
      "Run 802 of 1000 completed with return 249.2. Mean return over all episodes so far = 243.8 \n",
      "Run 803 of 1000 completed with return 268.0. Mean return over all episodes so far = 243.8 \n",
      "Run 804 of 1000 completed with return 150.2. Mean return over all episodes so far = 243.7 \n",
      "Run 805 of 1000 completed with return 245.9. Mean return over all episodes so far = 243.7 \n",
      "Run 806 of 1000 completed with return 250.5. Mean return over all episodes so far = 243.7 \n",
      "Run 807 of 1000 completed with return 236.6. Mean return over all episodes so far = 243.7 \n",
      "Run 808 of 1000 completed with return 233.6. Mean return over all episodes so far = 243.7 \n",
      "Run 809 of 1000 completed with return 138.4. Mean return over all episodes so far = 243.6 \n",
      "Run 810 of 1000 completed with return 255.4. Mean return over all episodes so far = 243.6 \n",
      "Run 811 of 1000 completed with return 242.3. Mean return over all episodes so far = 243.6 \n",
      "Run 812 of 1000 completed with return 274.1. Mean return over all episodes so far = 243.6 \n",
      "Run 813 of 1000 completed with return 286.9. Mean return over all episodes so far = 243.7 \n",
      "Run 814 of 1000 completed with return 275.5. Mean return over all episodes so far = 243.7 \n",
      "Run 815 of 1000 completed with return 271.3. Mean return over all episodes so far = 243.7 \n",
      "Run 816 of 1000 completed with return 256.1. Mean return over all episodes so far = 243.7 \n",
      "Run 817 of 1000 completed with return 275.8. Mean return over all episodes so far = 243.8 \n",
      "Run 818 of 1000 completed with return 242.1. Mean return over all episodes so far = 243.8 \n",
      "Run 819 of 1000 completed with return 238.9. Mean return over all episodes so far = 243.8 \n",
      "Run 820 of 1000 completed with return 246.0. Mean return over all episodes so far = 243.8 \n",
      "Run 821 of 1000 completed with return 268.5. Mean return over all episodes so far = 243.8 \n",
      "Run 822 of 1000 completed with return 244.9. Mean return over all episodes so far = 243.8 \n",
      "Run 823 of 1000 completed with return 270.7. Mean return over all episodes so far = 243.8 \n",
      "Run 824 of 1000 completed with return 247.3. Mean return over all episodes so far = 243.8 \n",
      "Run 825 of 1000 completed with return 236.4. Mean return over all episodes so far = 243.8 \n",
      "Run 826 of 1000 completed with return 267.5. Mean return over all episodes so far = 243.9 \n",
      "Run 827 of 1000 completed with return 275.4. Mean return over all episodes so far = 243.9 \n",
      "Run 828 of 1000 completed with return 256.9. Mean return over all episodes so far = 243.9 \n",
      "Run 829 of 1000 completed with return 268.6. Mean return over all episodes so far = 243.9 \n",
      "Run 830 of 1000 completed with return 241.2. Mean return over all episodes so far = 243.9 \n",
      "Run 831 of 1000 completed with return 107.7. Mean return over all episodes so far = 243.8 \n",
      "Run 832 of 1000 completed with return 299.9. Mean return over all episodes so far = 243.8 \n",
      "Run 833 of 1000 completed with return 162.8. Mean return over all episodes so far = 243.8 \n",
      "Run 834 of 1000 completed with return 260.2. Mean return over all episodes so far = 243.8 \n",
      "Run 835 of 1000 completed with return 226.8. Mean return over all episodes so far = 243.8 \n",
      "Run 836 of 1000 completed with return 263.7. Mean return over all episodes so far = 243.8 \n",
      "Run 837 of 1000 completed with return 260.5. Mean return over all episodes so far = 243.8 \n",
      "Run 838 of 1000 completed with return 261.5. Mean return over all episodes so far = 243.8 \n",
      "Run 839 of 1000 completed with return 237.9. Mean return over all episodes so far = 243.8 \n",
      "Run 840 of 1000 completed with return 248.0. Mean return over all episodes so far = 243.8 \n",
      "Run 841 of 1000 completed with return 222.2. Mean return over all episodes so far = 243.8 \n",
      "Run 842 of 1000 completed with return 248.6. Mean return over all episodes so far = 243.8 \n",
      "Run 843 of 1000 completed with return 241.3. Mean return over all episodes so far = 243.8 \n",
      "Run 844 of 1000 completed with return 278.4. Mean return over all episodes so far = 243.8 \n",
      "Run 845 of 1000 completed with return 191.4. Mean return over all episodes so far = 243.8 \n",
      "Run 846 of 1000 completed with return 257.7. Mean return over all episodes so far = 243.8 \n",
      "Run 847 of 1000 completed with return 264.1. Mean return over all episodes so far = 243.8 \n",
      "Run 848 of 1000 completed with return 248.0. Mean return over all episodes so far = 243.8 \n",
      "Run 849 of 1000 completed with return 221.0. Mean return over all episodes so far = 243.8 \n",
      "Run 850 of 1000 completed with return 249.3. Mean return over all episodes so far = 243.8 \n",
      "Run 851 of 1000 completed with return 285.7. Mean return over all episodes so far = 243.8 \n",
      "Run 852 of 1000 completed with return 277.6. Mean return over all episodes so far = 243.9 \n",
      "Run 853 of 1000 completed with return 246.3. Mean return over all episodes so far = 243.9 \n",
      "Run 854 of 1000 completed with return 260.4. Mean return over all episodes so far = 243.9 \n",
      "Run 855 of 1000 completed with return 236.5. Mean return over all episodes so far = 243.9 \n",
      "Run 856 of 1000 completed with return 238.2. Mean return over all episodes so far = 243.9 \n",
      "Run 857 of 1000 completed with return 229.2. Mean return over all episodes so far = 243.9 \n",
      "Run 858 of 1000 completed with return 237.4. Mean return over all episodes so far = 243.9 \n",
      "Run 859 of 1000 completed with return 266.6. Mean return over all episodes so far = 243.9 \n",
      "Run 860 of 1000 completed with return 241.1. Mean return over all episodes so far = 243.9 \n",
      "Run 861 of 1000 completed with return 250.5. Mean return over all episodes so far = 243.9 \n",
      "Run 862 of 1000 completed with return 241.8. Mean return over all episodes so far = 243.9 \n",
      "Run 863 of 1000 completed with return 244.0. Mean return over all episodes so far = 243.9 \n",
      "Run 864 of 1000 completed with return 221.3. Mean return over all episodes so far = 243.9 \n",
      "Run 865 of 1000 completed with return 253.1. Mean return over all episodes so far = 243.9 \n",
      "Run 866 of 1000 completed with return 122.9. Mean return over all episodes so far = 243.7 \n",
      "Run 867 of 1000 completed with return 263.0. Mean return over all episodes so far = 243.8 \n",
      "Run 868 of 1000 completed with return 260.1. Mean return over all episodes so far = 243.8 \n",
      "Run 869 of 1000 completed with return 252.9. Mean return over all episodes so far = 243.8 \n",
      "Run 870 of 1000 completed with return 263.5. Mean return over all episodes so far = 243.8 \n",
      "Run 871 of 1000 completed with return 121.3. Mean return over all episodes so far = 243.7 \n",
      "Run 872 of 1000 completed with return 295.1. Mean return over all episodes so far = 243.7 \n",
      "Run 873 of 1000 completed with return 257.2. Mean return over all episodes so far = 243.7 \n",
      "Run 874 of 1000 completed with return 282.7. Mean return over all episodes so far = 243.8 \n",
      "Run 875 of 1000 completed with return 243.3. Mean return over all episodes so far = 243.8 \n",
      "Run 876 of 1000 completed with return 254.5. Mean return over all episodes so far = 243.8 \n",
      "Run 877 of 1000 completed with return 271.3. Mean return over all episodes so far = 243.8 \n",
      "Run 878 of 1000 completed with return 257.0. Mean return over all episodes so far = 243.9 \n",
      "Run 879 of 1000 completed with return 238.4. Mean return over all episodes so far = 243.8 \n",
      "Run 880 of 1000 completed with return 270.7. Mean return over all episodes so far = 243.9 \n",
      "Run 881 of 1000 completed with return 263.9. Mean return over all episodes so far = 243.9 \n",
      "Run 882 of 1000 completed with return 8.6  . Mean return over all episodes so far = 243.6 \n",
      "Run 883 of 1000 completed with return 275.9. Mean return over all episodes so far = 243.7 \n",
      "Run 884 of 1000 completed with return 250.9. Mean return over all episodes so far = 243.7 \n",
      "Run 885 of 1000 completed with return 243.7. Mean return over all episodes so far = 243.7 \n",
      "Run 886 of 1000 completed with return 251.9. Mean return over all episodes so far = 243.7 \n",
      "Run 887 of 1000 completed with return 268.0. Mean return over all episodes so far = 243.7 \n",
      "Run 888 of 1000 completed with return 245.0. Mean return over all episodes so far = 243.7 \n",
      "Run 889 of 1000 completed with return 262.9. Mean return over all episodes so far = 243.7 \n",
      "Run 890 of 1000 completed with return 227.2. Mean return over all episodes so far = 243.7 \n",
      "Run 891 of 1000 completed with return 259.5. Mean return over all episodes so far = 243.7 \n",
      "Run 892 of 1000 completed with return 240.2. Mean return over all episodes so far = 243.7 \n",
      "Run 893 of 1000 completed with return 247.9. Mean return over all episodes so far = 243.7 \n",
      "Run 894 of 1000 completed with return 256.6. Mean return over all episodes so far = 243.8 \n",
      "Run 895 of 1000 completed with return 232.8. Mean return over all episodes so far = 243.7 \n",
      "Run 896 of 1000 completed with return 271.3. Mean return over all episodes so far = 243.8 \n",
      "Run 897 of 1000 completed with return 260.4. Mean return over all episodes so far = 243.8 \n",
      "Run 898 of 1000 completed with return 272.4. Mean return over all episodes so far = 243.8 \n",
      "Run 899 of 1000 completed with return 218.7. Mean return over all episodes so far = 243.8 \n",
      "Run 900 of 1000 completed with return 260.7. Mean return over all episodes so far = 243.8 \n",
      "Run 901 of 1000 completed with return 249.4. Mean return over all episodes so far = 243.8 \n",
      "Run 902 of 1000 completed with return 282.8. Mean return over all episodes so far = 243.9 \n",
      "Run 903 of 1000 completed with return 244.4. Mean return over all episodes so far = 243.9 \n",
      "Run 904 of 1000 completed with return 242.5. Mean return over all episodes so far = 243.9 \n",
      "Run 905 of 1000 completed with return 228.0. Mean return over all episodes so far = 243.8 \n",
      "Run 906 of 1000 completed with return 268.5. Mean return over all episodes so far = 243.9 \n",
      "Run 907 of 1000 completed with return 254.0. Mean return over all episodes so far = 243.9 \n",
      "Run 908 of 1000 completed with return 245.7. Mean return over all episodes so far = 243.9 \n",
      "Run 909 of 1000 completed with return 255.7. Mean return over all episodes so far = 243.9 \n",
      "Run 910 of 1000 completed with return 279.2. Mean return over all episodes so far = 243.9 \n",
      "Run 911 of 1000 completed with return 254.1. Mean return over all episodes so far = 243.9 \n",
      "Run 912 of 1000 completed with return -120.8. Mean return over all episodes so far = 243.5 \n",
      "Run 913 of 1000 completed with return 244.7. Mean return over all episodes so far = 243.5 \n",
      "Run 914 of 1000 completed with return 263.5. Mean return over all episodes so far = 243.6 \n",
      "Run 915 of 1000 completed with return 146.1. Mean return over all episodes so far = 243.5 \n",
      "Run 916 of 1000 completed with return 256.7. Mean return over all episodes so far = 243.5 \n",
      "Run 917 of 1000 completed with return 230.1. Mean return over all episodes so far = 243.5 \n",
      "Run 918 of 1000 completed with return 230.4. Mean return over all episodes so far = 243.4 \n",
      "Run 919 of 1000 completed with return 253.1. Mean return over all episodes so far = 243.5 \n",
      "Run 920 of 1000 completed with return 290.6. Mean return over all episodes so far = 243.5 \n",
      "Run 921 of 1000 completed with return 268.2. Mean return over all episodes so far = 243.5 \n",
      "Run 922 of 1000 completed with return 242.6. Mean return over all episodes so far = 243.5 \n",
      "Run 923 of 1000 completed with return 243.9. Mean return over all episodes so far = 243.5 \n",
      "Run 924 of 1000 completed with return 141.3. Mean return over all episodes so far = 243.4 \n",
      "Run 925 of 1000 completed with return 241.0. Mean return over all episodes so far = 243.4 \n",
      "Run 926 of 1000 completed with return 263.8. Mean return over all episodes so far = 243.4 \n",
      "Run 927 of 1000 completed with return 211.1. Mean return over all episodes so far = 243.4 \n",
      "Run 928 of 1000 completed with return 242.7. Mean return over all episodes so far = 243.4 \n",
      "Run 929 of 1000 completed with return 232.4. Mean return over all episodes so far = 243.4 \n",
      "Run 930 of 1000 completed with return 249.0. Mean return over all episodes so far = 243.4 \n",
      "Run 931 of 1000 completed with return 106.7. Mean return over all episodes so far = 243.3 \n",
      "Run 932 of 1000 completed with return 249.5. Mean return over all episodes so far = 243.3 \n",
      "Run 933 of 1000 completed with return 262.1. Mean return over all episodes so far = 243.3 \n",
      "Run 934 of 1000 completed with return 245.6. Mean return over all episodes so far = 243.3 \n",
      "Run 935 of 1000 completed with return 269.7. Mean return over all episodes so far = 243.3 \n",
      "Run 936 of 1000 completed with return 259.4. Mean return over all episodes so far = 243.3 \n",
      "Run 937 of 1000 completed with return 46.6 . Mean return over all episodes so far = 243.1 \n",
      "Run 938 of 1000 completed with return 250.6. Mean return over all episodes so far = 243.1 \n",
      "Run 939 of 1000 completed with return 304.3. Mean return over all episodes so far = 243.2 \n",
      "Run 940 of 1000 completed with return 262.3. Mean return over all episodes so far = 243.2 \n",
      "Run 941 of 1000 completed with return 261.7. Mean return over all episodes so far = 243.2 \n",
      "Run 942 of 1000 completed with return 281.5. Mean return over all episodes so far = 243.3 \n",
      "Run 943 of 1000 completed with return 249.3. Mean return over all episodes so far = 243.3 \n",
      "Run 944 of 1000 completed with return 274.7. Mean return over all episodes so far = 243.3 \n",
      "Run 945 of 1000 completed with return 244.9. Mean return over all episodes so far = 243.3 \n",
      "Run 946 of 1000 completed with return 260.0. Mean return over all episodes so far = 243.3 \n",
      "Run 947 of 1000 completed with return 266.2. Mean return over all episodes so far = 243.4 \n",
      "Run 948 of 1000 completed with return 280.0. Mean return over all episodes so far = 243.4 \n",
      "Run 949 of 1000 completed with return 269.5. Mean return over all episodes so far = 243.4 \n",
      "Run 950 of 1000 completed with return 238.0. Mean return over all episodes so far = 243.4 \n",
      "Run 951 of 1000 completed with return 257.1. Mean return over all episodes so far = 243.4 \n",
      "Run 952 of 1000 completed with return 265.5. Mean return over all episodes so far = 243.5 \n",
      "Run 953 of 1000 completed with return 261.9. Mean return over all episodes so far = 243.5 \n",
      "Run 954 of 1000 completed with return 266.0. Mean return over all episodes so far = 243.5 \n",
      "Run 955 of 1000 completed with return 249.7. Mean return over all episodes so far = 243.5 \n",
      "Run 956 of 1000 completed with return 256.5. Mean return over all episodes so far = 243.5 \n",
      "Run 957 of 1000 completed with return 254.5. Mean return over all episodes so far = 243.5 \n",
      "Run 958 of 1000 completed with return 249.1. Mean return over all episodes so far = 243.5 \n",
      "Run 959 of 1000 completed with return 235.4. Mean return over all episodes so far = 243.5 \n",
      "Run 960 of 1000 completed with return 251.9. Mean return over all episodes so far = 243.5 \n",
      "Run 961 of 1000 completed with return 253.2. Mean return over all episodes so far = 243.5 \n",
      "Run 962 of 1000 completed with return 265.3. Mean return over all episodes so far = 243.6 \n",
      "Run 963 of 1000 completed with return 238.0. Mean return over all episodes so far = 243.6 \n",
      "Run 964 of 1000 completed with return 227.5. Mean return over all episodes so far = 243.5 \n",
      "Run 965 of 1000 completed with return 253.7. Mean return over all episodes so far = 243.6 \n",
      "Run 966 of 1000 completed with return 292.9. Mean return over all episodes so far = 243.6 \n",
      "Run 967 of 1000 completed with return 276.7. Mean return over all episodes so far = 243.6 \n",
      "Run 968 of 1000 completed with return 253.7. Mean return over all episodes so far = 243.7 \n",
      "Run 969 of 1000 completed with return 234.5. Mean return over all episodes so far = 243.6 \n",
      "Run 970 of 1000 completed with return 245.1. Mean return over all episodes so far = 243.6 \n",
      "Run 971 of 1000 completed with return 213.1. Mean return over all episodes so far = 243.6 \n",
      "Run 972 of 1000 completed with return 241.2. Mean return over all episodes so far = 243.6 \n",
      "Run 973 of 1000 completed with return 257.9. Mean return over all episodes so far = 243.6 \n",
      "Run 974 of 1000 completed with return 244.9. Mean return over all episodes so far = 243.6 \n",
      "Run 975 of 1000 completed with return 255.3. Mean return over all episodes so far = 243.6 \n",
      "Run 976 of 1000 completed with return 265.1. Mean return over all episodes so far = 243.7 \n",
      "Run 977 of 1000 completed with return 256.7. Mean return over all episodes so far = 243.7 \n",
      "Run 978 of 1000 completed with return 264.5. Mean return over all episodes so far = 243.7 \n",
      "Run 979 of 1000 completed with return 255.1. Mean return over all episodes so far = 243.7 \n",
      "Run 980 of 1000 completed with return 254.9. Mean return over all episodes so far = 243.7 \n",
      "Run 981 of 1000 completed with return 261.3. Mean return over all episodes so far = 243.7 \n",
      "Run 982 of 1000 completed with return 239.7. Mean return over all episodes so far = 243.7 \n",
      "Run 983 of 1000 completed with return 155.8. Mean return over all episodes so far = 243.6 \n",
      "Run 984 of 1000 completed with return 259.3. Mean return over all episodes so far = 243.7 \n",
      "Run 985 of 1000 completed with return 304.0. Mean return over all episodes so far = 243.7 \n",
      "Run 986 of 1000 completed with return 243.0. Mean return over all episodes so far = 243.7 \n",
      "Run 987 of 1000 completed with return 258.3. Mean return over all episodes so far = 243.7 \n",
      "Run 988 of 1000 completed with return 255.5. Mean return over all episodes so far = 243.7 \n",
      "Run 989 of 1000 completed with return 243.1. Mean return over all episodes so far = 243.7 \n",
      "Run 990 of 1000 completed with return 223.0. Mean return over all episodes so far = 243.7 \n",
      "Run 991 of 1000 completed with return 289.9. Mean return over all episodes so far = 243.8 \n",
      "Run 992 of 1000 completed with return 245.6. Mean return over all episodes so far = 243.8 \n",
      "Run 993 of 1000 completed with return 24.2 . Mean return over all episodes so far = 243.6 \n",
      "Run 994 of 1000 completed with return 199.4. Mean return over all episodes so far = 243.5 \n",
      "Run 995 of 1000 completed with return 269.9. Mean return over all episodes so far = 243.5 \n",
      "Run 996 of 1000 completed with return 264.3. Mean return over all episodes so far = 243.6 \n",
      "Run 997 of 1000 completed with return 286.7. Mean return over all episodes so far = 243.6 \n",
      "Run 998 of 1000 completed with return 247.1. Mean return over all episodes so far = 243.6 \n",
      "Run 999 of 1000 completed with return 254.0. Mean return over all episodes so far = 243.6 \n",
      "Run 1000 of 1000 completed with return 242.3. Mean return over all episodes so far = 243.6 \n"
     ]
    }
   ],
   "source": [
    "# Define settings directly for Jupyter or script-based use\n",
    "output_filename = 'my_agent.tar'\n",
    "output_filename_training_data = 'my_agent_training_data.h5'\n",
    "output_filename_time = 'my_agent_execution_time.txt'\n",
    "N = 1000  # Number of simulations\n",
    "verbose = True\n",
    "overwrite = True\n",
    "use_dqn = True  # Set to True for DQN, False for Actor-Critic\n",
    "use_ddqn = False  # Set to True for Double DQN (overrides DQN)\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Check if files already exist (if overwrite is False)\n",
    "if not overwrite:\n",
    "    error_msg = (\"File {0} already exists. If you want to overwrite\"\n",
    "                 \" that file, please set overwrite = True.\")\n",
    "    if os.path.exists(output_filename):\n",
    "        raise RuntimeError(error_msg.format(output_filename))\n",
    "    if os.path.exists(output_filename_trajs):\n",
    "        raise RuntimeError(error_msg.format(output_filename_trajs))\n",
    "\n",
    "def run_and_save_simulations(env, input_filename, output_filename, N=1000, use_dqn=False):\n",
    "    \"\"\"\n",
    "    Run simulations using a trained agent and save the results.\n",
    "    \"\"\"\n",
    "    # Load trained model\n",
    "    input_dictionary = torch.load(open(input_filename, 'rb'))\n",
    "    dict_keys = np.array(list(input_dictionary.keys())).astype(int)\n",
    "    max_index = np.max(dict_keys)\n",
    "    input_dictionary = input_dictionary[max_index]  # Get the latest model state\n",
    "\n",
    "    # Instantiate agent\n",
    "    parameters = input_dictionary['parameters']\n",
    "    if use_dqn:\n",
    "        my_agent = dqn(parameters=parameters)  # Make sure 'dqn' is defined in agent_class.py\n",
    "    else:\n",
    "        my_agent = actor_critic(parameters=parameters)  # Make sure 'actor_critic' is defined in agent_class.py\n",
    "    \n",
    "    my_agent.load_state(state=input_dictionary)  # Ensure load_state is implemented in your agent class\n",
    "\n",
    "    # Run simulations\n",
    "    durations = []\n",
    "    returns = []\n",
    "    status_string = (\"Run {0} of {1} completed with return {2:<5.1f}. Mean \"\n",
    "                     \"return over all episodes so far = {3:<6.1f}\")\n",
    "\n",
    "    for i in range(N):\n",
    "        state, info = env.reset()\n",
    "        episode_return = 0.\n",
    "\n",
    "        for n in itertools.count():\n",
    "            action = my_agent.act(state)\n",
    "            state, step_reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_return += step_reward\n",
    "\n",
    "            if done:\n",
    "                durations.append(n + 1)\n",
    "                returns.append(episode_return)\n",
    "\n",
    "                if verbose:\n",
    "                    print(status_string.format(i + 1, N, episode_return,\n",
    "                                               np.mean(np.array(returns))))\n",
    "                break\n",
    "\n",
    "    # Save results to file\n",
    "    dictionary = {'returns': np.array(returns),\n",
    "                  'durations': np.array(durations),\n",
    "                  'input_file': input_filename,\n",
    "                  'N': N}\n",
    "\n",
    "    with h5py.File(output_filename, 'w') as hf:\n",
    "        for key, value in dictionary.items():\n",
    "            hf.create_dataset(str(key), data=value)\n",
    "\n",
    "# Run the simulation\n",
    "run_and_save_simulations(env=env,\n",
    "                         input_filename=output_filename,\n",
    "                         output_filename=output_filename_trajs,\n",
    "                         N=N,\n",
    "                         use_dqn=use_dqn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4865b180",
   "metadata": {},
   "source": [
    "## Visualizing Training and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4fcb012",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function for running averages\n",
    "def running_mean(x, N=20):\n",
    "    x_out = np.zeros(len(x) - N, dtype=float)\n",
    "    for i in range(len(x) - N):\n",
    "        x_out[i] = np.mean(x[i:i+N+1])\n",
    "    return x_out\n",
    "\n",
    "# Define a function for plotting returns and durations\n",
    "def plot_returns_and_durations(training_results, filename=None):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(5, 8))\n",
    "    fig.subplots_adjust(hspace=0.0001)\n",
    "    #\n",
    "    # Return as a function of episode\n",
    "    ax = axes[0]\n",
    "    x = training_results['epsiode_returns']\n",
    "    t = np.arange(len(x)) + 1\n",
    "    #\n",
    "    ax.plot(t, x, label='training', color='dodgerblue',)\n",
    "    # Add running mean\n",
    "    x = running_mean(x=x, N=20)\n",
    "    t = np.arange(len(x)) + 20\n",
    "    ax.plot(t, x, color='black', label='running mean')\n",
    "    #\n",
    "    ax.axhline(230, ls='--', label='230', color='red')\n",
    "    #\n",
    "    ax.set_ylim(-499, 350)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xlim(0, len(t) + 100)\n",
    "    ax.set_xlabel(r'episode')\n",
    "    ax.set_ylabel(r'return')\n",
    "    #\n",
    "    ax = axes[1]\n",
    "    x = training_results['episode_durations']\n",
    "    t = np.arange(len(x)) + 1\n",
    "    #\n",
    "    ax.plot(t, x, label='training', color='dodgerblue',)\n",
    "    # Add running mean\n",
    "    x = running_mean(x=x, N=20)\n",
    "    t = np.arange(len(x)) + 20\n",
    "    ax.plot(t, x, color='black', label='running mean')\n",
    "    #\n",
    "    ax.axhline(1200, ls='--', label='230', color='red')\n",
    "    #\n",
    "    ax.set_ylim(0, 1100)\n",
    "    ax.set_xlim(0, len(t) + 100)\n",
    "    ax.set_xlabel(r'episode')\n",
    "    ax.set_ylabel(r'duration')\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1., 1.35), framealpha=0.95, fontsize=18)\n",
    "    #\n",
    "    plt.show()\n",
    "    if filename:\n",
    "        fig.savefig(filename, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62809a34",
   "metadata": {},
   "source": [
    "## Running the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "626352d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is the issue that the game window freezes when running gym games \n",
    "# in jupyter notebooks, see https://github.com/openai/gym/issues/2433\n",
    "# We here use the fix from that website, which is to use the following\n",
    "# wrapper class:\n",
    "class PyGameWrapper(gym.Wrapper):\n",
    "    def render(self, **kwargs):\n",
    "        retval = self.env.render( **kwargs)\n",
    "        for event in pygame.event.get():\n",
    "            pass\n",
    "        return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d8e44d5-211a-4c7c-9e13-462bd2e808c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: duration = 227, total return = 263.409\n",
      "Run 2: duration = 446, total return = 213.448\n",
      "Run 3: duration = 238, total return = 287.115\n",
      "Run 4: duration = 316, total return = 271.227\n",
      "Run 5: duration = 1000, total return =  93.743\n"
     ]
    }
   ],
   "source": [
    "# Create a wrapped environment\n",
    "env = PyGameWrapper(gym.make('LunarLander-v2',render_mode='human'))\n",
    "\n",
    "N_episodes = 5\n",
    "\n",
    "result_string = 'Run {0}: duration = {1}, total return = {2:7.3f}'\n",
    "\n",
    "for j in range(N_episodes):\n",
    "    state, info = env.reset()\n",
    "\n",
    "    total_reward = 0\n",
    "    for i in itertools.count():\n",
    "        #env.render()\n",
    "\n",
    "        action = my_agent.act(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print(result_string.format(j+1,i+1,total_reward))\n",
    "            break\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7642d8c-640e-49be-b23a-68c6fdccee5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: moviepy in c:\\users\\sanke\\appdata\\roaming\\python\\python312\\site-packages (2.1.1)\n",
      "Requirement already satisfied: decorator<6.0,>=4.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from moviepy) (5.1.1)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from moviepy) (2.33.1)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\users\\sanke\\appdata\\roaming\\python\\python312\\site-packages (from moviepy) (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.25.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from moviepy) (1.26.4)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\sanke\\appdata\\roaming\\python\\python312\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: python-dotenv>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from moviepy) (0.21.0)\n",
      "Requirement already satisfied: pillow<11.0,>=9.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from moviepy) (10.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (69.5.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from proglog<=1.0.0->moviepy) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->proglog<=1.0.0->moviepy) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f449280-039e-453f-b8b9-d77c593ef948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: duration = 942, total return = 264.225\n",
      "Run 2: duration = 293, total return = 271.112\n",
      "Run 3: duration = 232, total return = 246.154\n",
      "Run 4: duration = 249, total return = 300.294\n",
      "Run 5: duration = 236, total return = 236.408\n",
      "MoviePy - Building video ./video.mp4.\n",
      "MoviePy - Writing video ./video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready ./video.mp4\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers.monitoring import video_recorder\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "video = video_recorder.VideoRecorder(env, './video.mp4'.format())\n",
    "\n",
    "N_episodes = 5\n",
    "\n",
    "result_string = 'Run {0}: duration = {1}, total return = {2:7.3f}'\n",
    "\n",
    "for j in range(N_episodes):\n",
    "    state, info = env.reset()\n",
    "\n",
    "    total_reward = 0\n",
    "    for i in itertools.count():\n",
    "        video.capture_frame()\n",
    "\n",
    "        action = my_agent.act(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print(result_string.format(j+1,i+1,total_reward))\n",
    "            break\n",
    "\n",
    "video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "395a5439-9566-4cb9-9e70-6ea2ac805071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: duration = 267, total return = 286.089\n",
      "Run 2: duration = 225, total return = 261.430\n",
      "Run 3: duration = 227, total return = 260.387\n",
      "Run 4: duration = 268, total return = 265.652\n",
      "Run 5: duration = 384, total return = 230.082\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Create a directory to save the frames\n",
    "frames_dir = './frames'\n",
    "os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "\n",
    "N_episodes = 5\n",
    "result_string = 'Run {0}: duration = {1}, total return = {2:7.3f}'\n",
    "\n",
    "for j in range(N_episodes):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for i in itertools.count():\n",
    "        # Capture the frame as an image\n",
    "        frame = env.render()\n",
    "        frame_image = Image.fromarray(frame)\n",
    "        frame_path = os.path.join(frames_dir, f\"episode_{j+1}_step_{i+1}.png\")\n",
    "        frame_image.save(frame_path)\n",
    "\n",
    "        # Perform agent action\n",
    "        action = my_agent.act(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print(result_string.format(j+1, i+1, total_reward))\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92d627-bfe2-4879-8ddf-0aa62e2cacde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
